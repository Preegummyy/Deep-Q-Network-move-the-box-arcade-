{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f38cd47-c008-4eb6-a8eb-9601b063f26f",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb7a8d6-3ff9-462e-8511-2ba07d211d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a082ee2-99d9-418c-baec-00d836359166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359b594f-404e-4939-9839-cca7e1af29d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79c0a30-76ec-4114-9fb1-deb641636532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46256d99-b184-4bcd-99c1-08c825388915",
   "metadata": {},
   "source": [
    "# set pytorch gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064660a8-e343-42fc-9bd0-1e66f4e71934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099261cf-6c00-4c98-b746-5cc82e3036a6",
   "metadata": {},
   "source": [
    "# notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cf607-7ee3-457d-bc76-9fcebfb11a0c",
   "metadata": {},
   "source": [
    "## bit definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8e1010-870e-4461-b013-149b902ecf24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7908ba1-1755-4749-aca6-c51e69b72411",
   "metadata": {},
   "source": [
    "| box_value | box_name     |\n",
    "|-----------|--------------|\n",
    "| 0         | Empty        |\n",
    "| 1         | BrownWood    |\n",
    "| 2         | RedWood      |\n",
    "| 3         | GreenWood    |\n",
    "| 4         | BrownLeather |\n",
    "| 5         | BrownPaper   |\n",
    "| 6         | BlueSteel    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193c47c-5282-428c-8b3b-bf400aa5cc7d",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472d8f5d-220f-4ddc-9b7f-326a9692dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveTheBoxGameEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_height=9,\n",
    "        board_width=7,\n",
    "        unique_box_type=6,\n",
    "        p_add_3_box=0.77,\n",
    "        debug=False,\n",
    "    ):\n",
    "\n",
    "        # init parameter\n",
    "        self.board_height = board_height\n",
    "        self.board_width = board_width\n",
    "        self.unique_box_type = unique_box_type\n",
    "        self.p_add_3_box = p_add_3_box\n",
    "        self.debug = debug\n",
    "\n",
    "        # init color_map\n",
    "        box_color_map_list = (\n",
    "            np.array(\n",
    "                [\n",
    "                    [255, 255, 255],\n",
    "                    [169, 133, 92],\n",
    "                    [183, 71, 34],\n",
    "                    [156, 164, 52],\n",
    "                    [102, 65, 38],\n",
    "                    [189, 140, 61],\n",
    "                    [149, 165, 168],\n",
    "                ]\n",
    "            )\n",
    "            / 255\n",
    "        )\n",
    "        self.box_color_map = LinearSegmentedColormap.from_list(\n",
    "            \"box_color_map\",\n",
    "            box_color_map_list,\n",
    "            box_color_map_list.shape[0],\n",
    "        )\n",
    "\n",
    "        # init game\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_score = 0\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def createBoard(self):\n",
    "        self.board = np.zeros(\n",
    "            (self.board_height, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def createAboveBoard(self):\n",
    "        self.above_board = np.zeros(\n",
    "            (1, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def addAboveBox(self):\n",
    "        number_of_box_above = np.random.choice(\n",
    "            [3, 4], p=[self.p_add_3_box, 1 - self.p_add_3_box]\n",
    "        )\n",
    "        box_value = np.random.randint(\n",
    "            low=1,\n",
    "            high=self.unique_box_type + 1,\n",
    "            size=number_of_box_above,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox = np.zeros(\n",
    "            self.board_width,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox[:number_of_box_above] = 1\n",
    "        aboveBox[aboveBox == 1] = box_value\n",
    "        np.random.shuffle(aboveBox)\n",
    "        self.above_board = aboveBox.reshape((1, self.board_width))\n",
    "\n",
    "    def aboveIncludeFall(self):\n",
    "        is_gameover = self.checkIsGameover()\n",
    "        if is_gameover:\n",
    "            return is_gameover\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        self.above_board = temp_board[-1].reshape((1, self.board_width)).copy()\n",
    "        self.board = temp_board[:-1].copy()\n",
    "        self.addAboveBox()\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        if self.checkIsEmptyBoard():\n",
    "            return self.aboveIncludeFall()\n",
    "        return False\n",
    "\n",
    "    # game mechanism\n",
    "    def fall(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        for y in range(self.board_width):\n",
    "            x = 0\n",
    "            is_fall = False\n",
    "            while x < temp_board.shape[0] - 1:\n",
    "                # skip floor and run loop\n",
    "                x += 1\n",
    "                # check below is empty\n",
    "                if not self.checkIsEmpty(temp_board[x, y]) and self.checkIsEmpty(\n",
    "                    temp_board[x - 1, y]\n",
    "                ):\n",
    "                    # fall\n",
    "                    temp_board[x - 1, y] = temp_board[x, y].copy()\n",
    "                    temp_board[x, y] = 0\n",
    "                    is_fall = True\n",
    "\n",
    "                # reset and fall again\n",
    "                if is_fall and (x >= temp_board.shape[0] - 1):\n",
    "                    x = 0\n",
    "                    is_fall = False\n",
    "        return temp_board\n",
    "\n",
    "    def remove(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            self.displayTempBoard(temp_board)\n",
    "        remove_list = self.check(temp_board)\n",
    "        numbers_of_remove = np.unique(remove_list, axis=0).shape[0]\n",
    "        if self.debug:\n",
    "            print(f\"remove_list : {remove_list}\")\n",
    "            print(f\"number of remove box (remove): {numbers_of_remove}\")\n",
    "        numbers_of_remove_recursive = []\n",
    "        if len(remove_list) > 0:\n",
    "            for remove_index in remove_list:\n",
    "                temp_board[remove_index[0], remove_index[1]] = 0\n",
    "            temp_board = self.fall(temp_board)\n",
    "            temp_board, numbers_of_remove_recursive = self.remove(temp_board)\n",
    "        return temp_board, [numbers_of_remove] + numbers_of_remove_recursive\n",
    "\n",
    "    def check(self, temp_board):\n",
    "        remove_list = []\n",
    "        for x in range(temp_board.shape[0]):\n",
    "            remove_list = remove_list + self.checkRow(temp_board, x)\n",
    "\n",
    "        for y in range(temp_board.shape[1]):\n",
    "            remove_list = remove_list + self.checkCol(temp_board, y)\n",
    "        return remove_list\n",
    "\n",
    "    def checkRow(self, temp_board, x):\n",
    "        count = 1\n",
    "        last_found = temp_board[x, 0]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "\n",
    "        for y in range(1, temp_board.shape[1]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if y >= temp_board.shape[1] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x, y - 1]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x, y - i - 1])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def checkCol(self, temp_board, y):\n",
    "        count = 1\n",
    "        last_found = temp_board[0, y]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "        for x in range(1, temp_board.shape[0]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if x >= temp_board.shape[0] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x - 1, y]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x - i - 1, y])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def updateBoard(self):\n",
    "        temp_board = self.board.copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        temp_board, numbers_of_remove = self.remove(temp_board)\n",
    "        self.board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            print(f\"numbers_of_remove (updateBoard): {numbers_of_remove}\")\n",
    "        return self.calculatePoint(numbers_of_remove)\n",
    "\n",
    "    def calculatePoint(self, numbers_of_remove_box):\n",
    "        numbers_of_remove_box = np.array(numbers_of_remove_box)\n",
    "        points = []\n",
    "        for t, number_of_box in enumerate(numbers_of_remove_box):\n",
    "            if number_of_box > 0:\n",
    "                summation = 0\n",
    "                for i in range(0, number_of_box - 3 + 1):\n",
    "                    summation = summation + i\n",
    "                if t == 0:\n",
    "                    point = number_of_box + summation\n",
    "                else:\n",
    "                    main_point = numbers_of_remove_box[t] * np.sum(\n",
    "                        numbers_of_remove_box[:t]\n",
    "                    )\n",
    "                    point = main_point + (number_of_box - 3) + summation\n",
    "            else:\n",
    "                point = 0\n",
    "            points.append(point)\n",
    "        return np.sum(points)\n",
    "\n",
    "    # utility\n",
    "    def displayBoard(self):\n",
    "        # create virtual board\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height + 1, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        yticks_label = yticks_label.astype(np.str)\n",
    "        yticks_label[-1] = \"above\"\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def displayTempBoard(self, temp_board):\n",
    "        # create virtual board\n",
    "        temp_board = temp_board.copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def checkIsEmpty(self, box):\n",
    "        return box == 0\n",
    "\n",
    "    def checkIsSameColor(self, box1, box2):\n",
    "        return box1 == box2\n",
    "\n",
    "    def checkIsEmptyBoard(self):\n",
    "        return (self.board == 0).all()\n",
    "\n",
    "    # action\n",
    "    def findAllMove(self):\n",
    "        all_move_index = []\n",
    "        row_arange = np.arange(0, self.board_height, 1)\n",
    "        col_arange = np.arange(0, self.board_width, 1)\n",
    "        row_index, col_index = np.meshgrid(row_arange, col_arange)\n",
    "        point_index = np.vstack((row_index.reshape(-1), col_index.reshape(-1))).T\n",
    "\n",
    "        for index in point_index:\n",
    "\n",
    "            if index[1] < self.board_width - 1:\n",
    "                # move_right\n",
    "                move_right_index = np.array([index, index + np.array([0, 1])])\n",
    "                all_move_index.append(move_right_index)\n",
    "\n",
    "            if index[0] < self.board_height - 1:\n",
    "                # move_up\n",
    "                move_up_index = np.array([index, index + np.array([1, 0])])\n",
    "                all_move_index.append(move_up_index)\n",
    "\n",
    "        return np.array(all_move_index)\n",
    "\n",
    "    def findNotDuplicateMove(self):\n",
    "        all_move_index = self.findAllMove()\n",
    "\n",
    "        not_duplicate_move_index = []\n",
    "        not_duplicate_array_index = []\n",
    "        for i, move_index in enumerate(all_move_index):\n",
    "            point_a_index = move_index[0]\n",
    "            point_a_value = self.board[point_a_index[0], point_a_index[1]]\n",
    "\n",
    "            point_b_index = move_index[1]\n",
    "            point_b_value = self.board[point_b_index[0], point_b_index[1]]\n",
    "\n",
    "            if not self.checkIsSameColor(point_a_value, point_b_value):\n",
    "                not_duplicate_move_index.append(move_index)\n",
    "                not_duplicate_array_index.append(i)\n",
    "\n",
    "        return np.array(not_duplicate_array_index), np.array(not_duplicate_move_index)\n",
    "\n",
    "    def move(self, move_index):\n",
    "        before_cumulative_score = self.cumulative_score\n",
    "        # collect value\n",
    "        point_a_index = move_index[0]\n",
    "        point_a_value = self.board[point_a_index[0], point_a_index[1]].copy()\n",
    "\n",
    "        point_b_index = move_index[1]\n",
    "        point_b_value = self.board[point_b_index[0], point_b_index[1]].copy()\n",
    "\n",
    "        # move\n",
    "        self.board[point_a_index[0], point_a_index[1]] = point_b_value\n",
    "        self.board[point_b_index[0], point_b_index[1]] = point_a_value\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        is_gameover = self.aboveIncludeFall()\n",
    "        return (\n",
    "            self.getGameState(),\n",
    "            self.cumulative_score - before_cumulative_score,\n",
    "            is_gameover,\n",
    "        )\n",
    "\n",
    "    def checkIsGameover(self):\n",
    "        for y in range(self.board.shape[1]):\n",
    "            full_board = (self.board[:, y] != 0).all()\n",
    "            full_above_board = (self.above_board[:, y] != 0).all()\n",
    "            if full_board and full_above_board:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Reinforcement\n",
    "    def getGameState(self):\n",
    "        return np.vstack((self.board, self.above_board)).copy()\n",
    "\n",
    "    def play(self):\n",
    "        temp_board_history = []\n",
    "        move_history = []\n",
    "        score_history = []\n",
    "        for i in range(200):\n",
    "            temp_board = self.getGameState()\n",
    "            _, move_index = self.findNotDuplicateMove()\n",
    "            select_move_index = np.random.randint(move_index.shape[0])\n",
    "            move_history_str = f\"move {i+1} : move from {move_index[select_move_index,0,:]} to {move_index[select_move_index,1,:]}\"\n",
    "            _, round_score, is_gameover = self.move(move_index[select_move_index])\n",
    "            score_history.append(round_score)\n",
    "            temp_board_history.append(temp_board)\n",
    "            move_history.append(move_history_str)\n",
    "            if is_gameover:\n",
    "                break\n",
    "        return temp_board_history, move_history, score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e494b-9b47-40e3-b1f8-0d7c05d8ac40",
   "metadata": {},
   "source": [
    "# deep q learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5dc4db-307e-458f-9528-0eeef29dbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.linear1 = nn.Linear(576, 64)\n",
    "        self.linear2 = nn.Linear(64, 16)\n",
    "        self.linear3 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0c9b67-c662-45be-9dd1-a650f97666f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac87c1e-c5c6-4c61-b493-28212ac4ef08",
   "metadata": {},
   "source": [
    "# replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc783a2b-0e44-4411-b005-5d698a172420",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d2d51-9889-4322-b056-3bd49e73d2bc",
   "metadata": {},
   "source": [
    "# setup parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82524c32-1c64-4c06-94fb-d5cec2e63d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quick running\n",
    "reduce_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da74fb88-4eef-43a6-8666-c01649855a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning parameter\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 1000\n",
    "max_episode = 1000000 // reduce_step\n",
    "max_buffer_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "969ba331-3206-412e-8283-1d5a1d7e9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy parameter\n",
    "epsilon = 1.0\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "\n",
    "# epsilon greedy frame\n",
    "epsilon_random_frames = 2000000 // reduce_step\n",
    "epsilon_greedy_frames = 10000000 // reduce_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b9f48c-2e42-46b1-bfdc-8094ff58b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning parameter\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5dd7378-1d4f-46fa-83fb-12d162117bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning parameter\n",
    "update_after_actions = 1\n",
    "update_target_network = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f602c5-efd4-426b-9859-d648b465841d",
   "metadata": {},
   "source": [
    "# create object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854385c0-541d-4542-abf5-81c254ed6876",
   "metadata": {},
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d621f96-2cf7-4983-8b96-325dcf3cabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49d5afb-a252-4298-ad73-45936e924a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = env.findAllMove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb622094-83a8-41cd-b137-997af2622d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_list.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc81793-21da-4543-a458-395e0947c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_height = env.board_height\n",
    "board_width = env.board_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912cf90-202e-485c-b7d8-b4665a0e945f",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3989c8-269b-4563-b780-16e745edd07f",
   "metadata": {},
   "source": [
    "### policy net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd62851-5646-49a8-b1e6-25772613742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(board_height, board_width, num_actions).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a853891-ffb5-49bc-841a-933ba767811f",
   "metadata": {},
   "source": [
    "#### load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba04120-17cf-41c4-b172-34374274f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net.load_state_dict(torch.load(\"policy_net_move_select.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e4cb64-ad04-4a2d-ab79-34b79f17394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pre_s\\AppData\\Local\\Temp\\ipykernel_13452\\3626013848.py:7: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n",
      "C:\\Users\\pre_s\\AppData\\Local\\Temp\\ipykernel_13452\\3626013848.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=576, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (linear3): Linear(in_features=16, out_features=110, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496c51a-a5b5-4ca5-a730-13420c846d47",
   "metadata": {},
   "source": [
    "### target net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0d2863f-2b45-4ee8-938a-5e8eb64f8daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear1): Linear(in_features=576, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (linear3): Linear(in_features=16, out_features=110, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net = DQN(board_height, board_width, num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a0cab-38d8-4c6b-b026-3c3f339bc5a7",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3f468d0-8af5-4dbd-9885-889853a93d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    policy_net.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0ebb4-ea53-4ea3-884d-61d855e70758",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c5a2ef2-40b7-4bd5-8f44-4c5f02c2188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ec57c-8b9e-4ced-bfd1-d2469a9ff03d",
   "metadata": {},
   "source": [
    "## replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0979c095-c327-403f-97c1-bdf2534392e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(max_buffer_size)\n",
    "episode_reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ff84f-8bf3-48c0-a9d2-f069aa58a74d",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf4fedaa-f324-4f9c-b765-f307361a8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_break = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474ac94e-9f05-414e-a5b6-c2c75153236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970a6eb2-798e-42a5-8d77-5af82eb658cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c8c07-837c-4edd-873c-6fa13700d8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ffcb8b73244118a26e3ef4a19961cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 8.61 at episode 607, frame count 10000, action_str : random valid_action\n",
      "running reward: 7.94 at episode 1216, frame count 20000, action_str : random valid_action\n",
      "running reward: 10.27 at episode 1819, frame count 30000, action_str : random valid_action\n",
      "running reward: 9.85 at episode 2423, frame count 40000, action_str : random valid_action\n",
      "running reward: 10.22 at episode 3028, frame count 50000, action_str : random valid_action\n",
      "running reward: 12.18 at episode 3634, frame count 60000, action_str : random valid_action\n",
      "running reward: 9.16 at episode 4245, frame count 70000, action_str : random valid_action\n",
      "running reward: 11.04 at episode 4855, frame count 80000, action_str : random valid_action\n",
      "running reward: 10.31 at episode 5464, frame count 90000, action_str : random valid_action\n",
      "running reward: 12.19 at episode 6068, frame count 100000, action_str : random valid_action\n",
      "running reward: 12.08 at episode 6675, frame count 110000, action_str : random valid_action\n",
      "running reward: 9.63 at episode 7283, frame count 120000, action_str : random valid_action\n",
      "running reward: 10.07 at episode 7896, frame count 130000, action_str : random valid_action\n",
      "running reward: 8.81 at episode 8506, frame count 140000, action_str : random valid_action\n",
      "running reward: 9.49 at episode 9110, frame count 150000, action_str : random valid_action\n",
      "running reward: 10.66 at episode 9711, frame count 160000, action_str : random valid_action\n",
      "running reward: 9.81 at episode 10316, frame count 170000, action_str : random valid_action\n",
      "running reward: 9.59 at episode 10919, frame count 180000, action_str : random valid_action\n",
      "running reward: 10.11 at episode 11533, frame count 190000, action_str : random valid_action\n",
      "running reward: 10.55 at episode 12135, frame count 200000, action_str : random valid_action\n",
      "running reward: 9.07 at episode 12741, frame count 210000, action_str : random valid_action\n",
      "running reward: 10.83 at episode 13343, frame count 220000, action_str : random valid_action\n",
      "running reward: 9.23 at episode 13950, frame count 230000, action_str : random valid_action\n",
      "running reward: 11.09 at episode 14552, frame count 240000, action_str : random valid_action\n",
      "running reward: 9.73 at episode 15155, frame count 250000, action_str : random valid_action\n",
      "running reward: 10.93 at episode 15757, frame count 260000, action_str : random valid_action\n",
      "running reward: 10.06 at episode 16364, frame count 270000, action_str : random valid_action\n",
      "running reward: 10.64 at episode 16963, frame count 280000, action_str : random valid_action\n",
      "running reward: 8.70 at episode 17576, frame count 290000, action_str : random valid_action\n",
      "running reward: 11.55 at episode 18181, frame count 300000, action_str : random valid_action\n",
      "running reward: 9.12 at episode 18790, frame count 310000, action_str : random valid_action\n",
      "running reward: 11.43 at episode 19394, frame count 320000, action_str : random valid_action\n",
      "running reward: 11.63 at episode 19996, frame count 330000, action_str : random valid_action\n",
      "running reward: 9.42 at episode 20602, frame count 340000, action_str : random valid_action\n",
      "running reward: 8.34 at episode 21213, frame count 350000, action_str : random valid_action\n",
      "running reward: 8.16 at episode 21823, frame count 360000, action_str : random valid_action\n",
      "running reward: 10.32 at episode 22430, frame count 370000, action_str : random valid_action\n",
      "running reward: 9.56 at episode 23036, frame count 380000, action_str : random valid_action\n",
      "running reward: 10.38 at episode 23641, frame count 390000, action_str : random valid_action\n",
      "running reward: 12.46 at episode 24249, frame count 400000, action_str : random valid_action\n",
      "running reward: 10.27 at episode 24850, frame count 410000, action_str : random valid_action\n",
      "running reward: 8.87 at episode 25455, frame count 420000, action_str : network\n",
      "running reward: 9.37 at episode 26051, frame count 430000, action_str : random valid_action\n",
      "running reward: 9.35 at episode 26659, frame count 440000, action_str : network\n",
      "running reward: 10.63 at episode 27253, frame count 450000, action_str : random valid_action\n",
      "running reward: 9.57 at episode 27850, frame count 460000, action_str : network\n",
      "running reward: 10.24 at episode 28446, frame count 470000, action_str : random valid_action\n",
      "running reward: 9.01 at episode 29041, frame count 480000, action_str : network\n",
      "running reward: 9.96 at episode 29637, frame count 490000, action_str : network\n",
      "running reward: 8.58 at episode 30231, frame count 500000, action_str : random valid_action\n",
      "running reward: 9.34 at episode 30834, frame count 510000, action_str : random valid_action\n",
      "running reward: 9.27 at episode 31432, frame count 520000, action_str : random valid_action\n",
      "running reward: 9.54 at episode 32027, frame count 530000, action_str : random valid_action\n",
      "running reward: 10.16 at episode 32628, frame count 540000, action_str : random valid_action\n",
      "running reward: 10.59 at episode 33232, frame count 550000, action_str : random valid_action\n",
      "running reward: 9.29 at episode 33833, frame count 560000, action_str : random valid_action\n",
      "running reward: 9.73 at episode 34428, frame count 570000, action_str : random valid_action\n",
      "running reward: 9.18 at episode 35026, frame count 580000, action_str : random valid_action\n",
      "running reward: 10.01 at episode 35628, frame count 590000, action_str : random valid_action\n",
      "running reward: 10.32 at episode 36226, frame count 600000, action_str : random valid_action\n",
      "running reward: 8.88 at episode 36824, frame count 610000, action_str : random valid_action\n",
      "running reward: 8.09 at episode 37424, frame count 620000, action_str : random valid_action\n",
      "running reward: 9.39 at episode 38025, frame count 630000, action_str : random valid_action\n",
      "running reward: 10.17 at episode 38625, frame count 640000, action_str : random valid_action\n",
      "running reward: 9.53 at episode 39226, frame count 650000, action_str : network\n",
      "running reward: 10.12 at episode 39831, frame count 660000, action_str : random valid_action\n",
      "running reward: 10.32 at episode 40427, frame count 670000, action_str : network\n",
      "running reward: 10.13 at episode 41028, frame count 680000, action_str : random valid_action\n",
      "running reward: 9.46 at episode 41628, frame count 690000, action_str : random valid_action\n",
      "running reward: 8.84 at episode 42222, frame count 700000, action_str : random valid_action\n",
      "running reward: 9.22 at episode 42817, frame count 710000, action_str : random valid_action\n",
      "running reward: 12.49 at episode 43415, frame count 720000, action_str : random valid_action\n",
      "running reward: 10.22 at episode 44010, frame count 730000, action_str : random valid_action\n",
      "running reward: 8.87 at episode 44613, frame count 740000, action_str : random valid_action\n",
      "running reward: 10.71 at episode 45210, frame count 750000, action_str : random valid_action\n",
      "running reward: 8.17 at episode 45810, frame count 760000, action_str : random valid_action\n",
      "running reward: 8.07 at episode 46410, frame count 770000, action_str : random valid_action\n",
      "running reward: 9.60 at episode 47004, frame count 780000, action_str : random valid_action\n",
      "running reward: 8.50 at episode 47602, frame count 790000, action_str : random valid_action\n",
      "running reward: 9.98 at episode 48201, frame count 800000, action_str : network\n",
      "running reward: 9.30 at episode 48803, frame count 810000, action_str : random valid_action\n",
      "running reward: 10.48 at episode 49400, frame count 820000, action_str : network\n",
      "running reward: 8.86 at episode 50001, frame count 830000, action_str : random valid_action\n",
      "running reward: 9.80 at episode 50596, frame count 840000, action_str : network\n",
      "running reward: 9.27 at episode 51197, frame count 850000, action_str : network\n",
      "running reward: 8.71 at episode 51797, frame count 860000, action_str : random valid_action\n",
      "running reward: 8.66 at episode 52394, frame count 870000, action_str : random valid_action\n",
      "running reward: 9.94 at episode 52987, frame count 880000, action_str : random valid_action\n",
      "running reward: 8.97 at episode 53581, frame count 890000, action_str : random valid_action\n",
      "running reward: 8.97 at episode 54180, frame count 900000, action_str : random valid_action\n",
      "running reward: 9.74 at episode 54773, frame count 910000, action_str : random valid_action\n",
      "running reward: 9.35 at episode 55364, frame count 920000, action_str : random valid_action\n",
      "running reward: 8.48 at episode 55960, frame count 930000, action_str : random valid_action\n",
      "running reward: 9.93 at episode 56557, frame count 940000, action_str : random valid_action\n",
      "running reward: 8.41 at episode 57156, frame count 950000, action_str : network\n",
      "running reward: 9.29 at episode 57751, frame count 960000, action_str : random invalid_action\n",
      "running reward: 8.15 at episode 58348, frame count 970000, action_str : network\n",
      "running reward: 8.02 at episode 58936, frame count 980000, action_str : network\n",
      "running reward: 9.46 at episode 59529, frame count 990000, action_str : random valid_action\n",
      "running reward: 9.94 at episode 60123, frame count 1000000, action_str : random valid_action\n",
      "running reward: 9.36 at episode 60718, frame count 1010000, action_str : network\n",
      "running reward: 10.23 at episode 61309, frame count 1020000, action_str : network\n",
      "running reward: 8.63 at episode 61903, frame count 1030000, action_str : network\n",
      "running reward: 9.21 at episode 62495, frame count 1040000, action_str : random valid_action\n",
      "running reward: 8.88 at episode 63090, frame count 1050000, action_str : random valid_action\n",
      "running reward: 7.66 at episode 63687, frame count 1060000, action_str : network\n",
      "running reward: 8.71 at episode 64277, frame count 1070000, action_str : network\n",
      "running reward: 9.20 at episode 64871, frame count 1080000, action_str : network\n",
      "running reward: 8.29 at episode 65465, frame count 1090000, action_str : random valid_action\n",
      "running reward: 9.30 at episode 66060, frame count 1100000, action_str : network\n",
      "running reward: 10.49 at episode 66656, frame count 1110000, action_str : random valid_action\n",
      "running reward: 8.85 at episode 67250, frame count 1120000, action_str : random valid_action\n",
      "running reward: 10.54 at episode 67845, frame count 1130000, action_str : network\n",
      "running reward: 9.55 at episode 68434, frame count 1140000, action_str : random valid_action\n",
      "running reward: 7.96 at episode 69026, frame count 1150000, action_str : network\n",
      "running reward: 8.75 at episode 69624, frame count 1160000, action_str : random valid_action\n",
      "running reward: 10.05 at episode 70224, frame count 1170000, action_str : random valid_action\n",
      "running reward: 8.37 at episode 70823, frame count 1180000, action_str : random valid_action\n",
      "running reward: 9.85 at episode 71421, frame count 1190000, action_str : network\n",
      "running reward: 9.01 at episode 72016, frame count 1200000, action_str : network\n",
      "running reward: 9.80 at episode 72603, frame count 1210000, action_str : network\n",
      "running reward: 9.26 at episode 73196, frame count 1220000, action_str : network\n",
      "running reward: 8.71 at episode 73792, frame count 1230000, action_str : random valid_action\n",
      "running reward: 10.53 at episode 74384, frame count 1240000, action_str : network\n",
      "running reward: 9.45 at episode 74979, frame count 1250000, action_str : network\n",
      "running reward: 10.93 at episode 75566, frame count 1260000, action_str : network\n",
      "running reward: 9.53 at episode 76156, frame count 1270000, action_str : random valid_action\n",
      "running reward: 9.11 at episode 76745, frame count 1280000, action_str : network\n",
      "running reward: 8.83 at episode 77338, frame count 1290000, action_str : random valid_action\n",
      "running reward: 8.08 at episode 77929, frame count 1300000, action_str : network\n",
      "running reward: 10.00 at episode 78521, frame count 1310000, action_str : random valid_action\n",
      "running reward: 8.96 at episode 79112, frame count 1320000, action_str : network\n",
      "running reward: 9.16 at episode 79709, frame count 1330000, action_str : network\n",
      "running reward: 8.35 at episode 80301, frame count 1340000, action_str : random valid_action\n",
      "running reward: 9.61 at episode 80895, frame count 1350000, action_str : random valid_action\n",
      "running reward: 8.44 at episode 81491, frame count 1360000, action_str : network\n",
      "running reward: 11.34 at episode 82079, frame count 1370000, action_str : network\n",
      "running reward: 10.89 at episode 82668, frame count 1380000, action_str : network\n",
      "running reward: 10.70 at episode 83258, frame count 1390000, action_str : network\n",
      "running reward: 8.15 at episode 83849, frame count 1400000, action_str : network\n",
      "running reward: 8.53 at episode 84441, frame count 1410000, action_str : network\n",
      "running reward: 9.69 at episode 85040, frame count 1420000, action_str : network\n",
      "running reward: 8.80 at episode 85640, frame count 1430000, action_str : random valid_action\n",
      "running reward: 9.50 at episode 86236, frame count 1440000, action_str : network\n",
      "running reward: 7.95 at episode 86832, frame count 1450000, action_str : random valid_action\n",
      "running reward: 7.61 at episode 87424, frame count 1460000, action_str : network\n",
      "running reward: 8.95 at episode 88016, frame count 1470000, action_str : network\n",
      "running reward: 9.52 at episode 88608, frame count 1480000, action_str : random valid_action\n",
      "running reward: 8.09 at episode 89210, frame count 1490000, action_str : network\n",
      "running reward: 10.57 at episode 89801, frame count 1500000, action_str : network\n",
      "running reward: 9.19 at episode 90391, frame count 1510000, action_str : network\n",
      "running reward: 8.18 at episode 90983, frame count 1520000, action_str : random valid_action\n",
      "running reward: 8.55 at episode 91572, frame count 1530000, action_str : random valid_action\n",
      "running reward: 7.66 at episode 92157, frame count 1540000, action_str : network\n",
      "running reward: 8.01 at episode 92750, frame count 1550000, action_str : network\n",
      "running reward: 9.21 at episode 93338, frame count 1560000, action_str : random valid_action\n",
      "running reward: 8.22 at episode 93930, frame count 1570000, action_str : random valid_action\n",
      "running reward: 8.78 at episode 94526, frame count 1580000, action_str : random valid_action\n",
      "running reward: 8.00 at episode 95118, frame count 1590000, action_str : network\n",
      "running reward: 8.88 at episode 95713, frame count 1600000, action_str : random valid_action\n",
      "running reward: 8.60 at episode 96308, frame count 1610000, action_str : network\n",
      "running reward: 10.52 at episode 96897, frame count 1620000, action_str : random valid_action\n",
      "running reward: 6.91 at episode 97494, frame count 1630000, action_str : network\n",
      "running reward: 8.01 at episode 98090, frame count 1640000, action_str : network\n",
      "running reward: 7.50 at episode 98680, frame count 1650000, action_str : network\n",
      "running reward: 7.91 at episode 99279, frame count 1660000, action_str : network\n",
      "running reward: 9.94 at episode 99867, frame count 1670000, action_str : random valid_action\n",
      "running reward: 6.78 at episode 100464, frame count 1680000, action_str : random valid_action\n",
      "running reward: 7.48 at episode 101057, frame count 1690000, action_str : random valid_action\n",
      "running reward: 7.28 at episode 101657, frame count 1700000, action_str : network\n",
      "running reward: 10.16 at episode 102252, frame count 1710000, action_str : network\n",
      "running reward: 8.26 at episode 102842, frame count 1720000, action_str : network\n",
      "running reward: 9.89 at episode 103441, frame count 1730000, action_str : network\n",
      "running reward: 8.43 at episode 104031, frame count 1740000, action_str : network\n",
      "running reward: 7.38 at episode 104621, frame count 1750000, action_str : network\n",
      "running reward: 7.69 at episode 105208, frame count 1760000, action_str : random valid_action\n",
      "running reward: 9.00 at episode 105795, frame count 1770000, action_str : network\n",
      "running reward: 9.52 at episode 106381, frame count 1780000, action_str : network\n",
      "running reward: 7.93 at episode 106970, frame count 1790000, action_str : network\n",
      "running reward: 7.97 at episode 107555, frame count 1800000, action_str : network\n",
      "running reward: 7.62 at episode 108144, frame count 1810000, action_str : network\n",
      "running reward: 9.44 at episode 108735, frame count 1820000, action_str : random valid_action\n",
      "running reward: 8.07 at episode 109326, frame count 1830000, action_str : random valid_action\n",
      "running reward: 7.10 at episode 109914, frame count 1840000, action_str : network\n",
      "running reward: 7.36 at episode 110509, frame count 1850000, action_str : network\n",
      "running reward: 7.75 at episode 111093, frame count 1860000, action_str : network\n",
      "running reward: 8.12 at episode 111684, frame count 1870000, action_str : network\n",
      "running reward: 7.22 at episode 112277, frame count 1880000, action_str : random valid_action\n",
      "running reward: 9.49 at episode 112872, frame count 1890000, action_str : network\n",
      "running reward: 7.60 at episode 113463, frame count 1900000, action_str : network\n",
      "running reward: 8.90 at episode 114052, frame count 1910000, action_str : random valid_action\n",
      "running reward: 7.71 at episode 114641, frame count 1920000, action_str : network\n",
      "running reward: 7.82 at episode 115228, frame count 1930000, action_str : network\n",
      "running reward: 9.84 at episode 115813, frame count 1940000, action_str : network\n",
      "running reward: 6.75 at episode 116404, frame count 1950000, action_str : network\n",
      "running reward: 6.39 at episode 116985, frame count 1960000, action_str : network\n",
      "running reward: 6.54 at episode 117573, frame count 1970000, action_str : network\n",
      "running reward: 7.05 at episode 118157, frame count 1980000, action_str : network\n",
      "running reward: 8.51 at episode 118740, frame count 1990000, action_str : network\n",
      "running reward: 8.20 at episode 119322, frame count 2000000, action_str : network\n",
      "running reward: 7.88 at episode 119906, frame count 2010000, action_str : network\n",
      "running reward: 8.76 at episode 120489, frame count 2020000, action_str : network\n",
      "running reward: 10.61 at episode 121074, frame count 2030000, action_str : network\n",
      "running reward: 7.17 at episode 121658, frame count 2040000, action_str : network\n",
      "running reward: 8.40 at episode 122243, frame count 2050000, action_str : network\n",
      "running reward: 7.56 at episode 122833, frame count 2060000, action_str : network\n",
      "running reward: 7.97 at episode 123419, frame count 2070000, action_str : network\n",
      "running reward: 7.43 at episode 124009, frame count 2080000, action_str : network\n",
      "running reward: 7.16 at episode 124599, frame count 2090000, action_str : network\n",
      "running reward: 7.63 at episode 125191, frame count 2100000, action_str : network\n",
      "running reward: 8.49 at episode 125779, frame count 2110000, action_str : network\n",
      "running reward: 7.22 at episode 126366, frame count 2120000, action_str : network\n",
      "running reward: 7.94 at episode 126952, frame count 2130000, action_str : network\n",
      "running reward: 8.11 at episode 127534, frame count 2140000, action_str : network\n",
      "running reward: 8.35 at episode 128120, frame count 2150000, action_str : network\n",
      "running reward: 8.76 at episode 128704, frame count 2160000, action_str : network\n",
      "running reward: 9.43 at episode 129290, frame count 2170000, action_str : network\n",
      "running reward: 9.57 at episode 129877, frame count 2180000, action_str : network\n",
      "running reward: 8.98 at episode 130460, frame count 2190000, action_str : random valid_action\n",
      "running reward: 8.69 at episode 131038, frame count 2200000, action_str : network\n",
      "running reward: 9.10 at episode 131616, frame count 2210000, action_str : network\n",
      "running reward: 7.75 at episode 132203, frame count 2220000, action_str : network\n",
      "running reward: 8.12 at episode 132793, frame count 2230000, action_str : network\n",
      "running reward: 8.31 at episode 133379, frame count 2240000, action_str : random valid_action\n",
      "running reward: 7.28 at episode 133969, frame count 2250000, action_str : network\n",
      "running reward: 8.15 at episode 134559, frame count 2260000, action_str : network\n",
      "running reward: 7.78 at episode 135153, frame count 2270000, action_str : network\n",
      "running reward: 7.79 at episode 135734, frame count 2280000, action_str : network\n",
      "running reward: 7.77 at episode 136326, frame count 2290000, action_str : random valid_action\n",
      "running reward: 7.64 at episode 136909, frame count 2300000, action_str : network\n",
      "running reward: 9.76 at episode 137495, frame count 2310000, action_str : network\n",
      "running reward: 7.15 at episode 138085, frame count 2320000, action_str : network\n",
      "running reward: 9.87 at episode 138671, frame count 2330000, action_str : network\n",
      "running reward: 8.24 at episode 139255, frame count 2340000, action_str : network\n",
      "running reward: 8.42 at episode 139837, frame count 2350000, action_str : network\n",
      "running reward: 8.82 at episode 140416, frame count 2360000, action_str : network\n",
      "running reward: 8.44 at episode 140994, frame count 2370000, action_str : random valid_action\n",
      "running reward: 9.31 at episode 141576, frame count 2380000, action_str : network\n",
      "running reward: 8.71 at episode 142156, frame count 2390000, action_str : random valid_action\n",
      "running reward: 8.17 at episode 142737, frame count 2400000, action_str : network\n",
      "running reward: 9.02 at episode 143319, frame count 2410000, action_str : network\n",
      "running reward: 9.63 at episode 143900, frame count 2420000, action_str : random valid_action\n",
      "running reward: 8.17 at episode 144483, frame count 2430000, action_str : network\n",
      "running reward: 7.89 at episode 145072, frame count 2440000, action_str : network\n",
      "running reward: 6.94 at episode 145662, frame count 2450000, action_str : network\n",
      "running reward: 7.82 at episode 146253, frame count 2460000, action_str : network\n",
      "running reward: 9.22 at episode 146845, frame count 2470000, action_str : network\n",
      "running reward: 9.39 at episode 147435, frame count 2480000, action_str : network\n",
      "running reward: 8.30 at episode 148024, frame count 2490000, action_str : network\n",
      "running reward: 10.40 at episode 148610, frame count 2500000, action_str : network\n",
      "running reward: 7.05 at episode 149206, frame count 2510000, action_str : network\n",
      "running reward: 8.72 at episode 149794, frame count 2520000, action_str : network\n",
      "running reward: 8.58 at episode 150379, frame count 2530000, action_str : network\n",
      "running reward: 9.75 at episode 150969, frame count 2540000, action_str : network\n",
      "running reward: 9.20 at episode 151558, frame count 2550000, action_str : random valid_action\n",
      "running reward: 8.92 at episode 152144, frame count 2560000, action_str : network\n",
      "running reward: 7.94 at episode 152738, frame count 2570000, action_str : random valid_action\n",
      "running reward: 7.42 at episode 153325, frame count 2580000, action_str : network\n",
      "running reward: 8.21 at episode 153915, frame count 2590000, action_str : network\n",
      "running reward: 8.79 at episode 154505, frame count 2600000, action_str : network\n",
      "running reward: 8.34 at episode 155094, frame count 2610000, action_str : network\n",
      "running reward: 6.75 at episode 155687, frame count 2620000, action_str : network\n",
      "running reward: 8.96 at episode 156272, frame count 2630000, action_str : network\n",
      "running reward: 7.55 at episode 156864, frame count 2640000, action_str : network\n",
      "running reward: 8.49 at episode 157453, frame count 2650000, action_str : network\n",
      "running reward: 6.61 at episode 158045, frame count 2660000, action_str : network\n",
      "running reward: 8.38 at episode 158628, frame count 2670000, action_str : network\n",
      "running reward: 8.57 at episode 159210, frame count 2680000, action_str : random valid_action\n",
      "running reward: 7.27 at episode 159789, frame count 2690000, action_str : network\n",
      "running reward: 8.98 at episode 160375, frame count 2700000, action_str : network\n",
      "running reward: 8.65 at episode 160963, frame count 2710000, action_str : network\n",
      "running reward: 8.73 at episode 161552, frame count 2720000, action_str : network\n",
      "running reward: 9.03 at episode 162131, frame count 2730000, action_str : network\n",
      "running reward: 8.44 at episode 162715, frame count 2740000, action_str : network\n",
      "running reward: 9.67 at episode 163298, frame count 2750000, action_str : network\n",
      "running reward: 8.71 at episode 163883, frame count 2760000, action_str : network\n",
      "running reward: 8.14 at episode 164463, frame count 2770000, action_str : random valid_action\n",
      "running reward: 8.44 at episode 165035, frame count 2780000, action_str : network\n",
      "running reward: 7.17 at episode 165619, frame count 2790000, action_str : network\n",
      "running reward: 8.77 at episode 166202, frame count 2800000, action_str : network\n",
      "running reward: 7.43 at episode 166790, frame count 2810000, action_str : network\n",
      "running reward: 8.63 at episode 167364, frame count 2820000, action_str : network\n",
      "running reward: 10.08 at episode 167945, frame count 2830000, action_str : network\n",
      "running reward: 8.29 at episode 168527, frame count 2840000, action_str : network\n",
      "running reward: 7.84 at episode 169109, frame count 2850000, action_str : network\n",
      "running reward: 8.67 at episode 169691, frame count 2860000, action_str : random valid_action\n",
      "running reward: 10.10 at episode 170273, frame count 2870000, action_str : network\n",
      "running reward: 9.41 at episode 170854, frame count 2880000, action_str : network\n",
      "running reward: 8.49 at episode 171437, frame count 2890000, action_str : network\n",
      "running reward: 9.15 at episode 172014, frame count 2900000, action_str : network\n",
      "running reward: 7.31 at episode 172598, frame count 2910000, action_str : network\n",
      "running reward: 7.87 at episode 173182, frame count 2920000, action_str : network\n",
      "running reward: 9.98 at episode 173766, frame count 2930000, action_str : network\n",
      "running reward: 8.91 at episode 174349, frame count 2940000, action_str : network\n",
      "running reward: 8.63 at episode 174934, frame count 2950000, action_str : network\n",
      "running reward: 8.39 at episode 175522, frame count 2960000, action_str : network\n",
      "running reward: 9.76 at episode 176105, frame count 2970000, action_str : network\n",
      "running reward: 8.21 at episode 176689, frame count 2980000, action_str : network\n",
      "running reward: 7.70 at episode 177278, frame count 2990000, action_str : network\n",
      "running reward: 7.69 at episode 177864, frame count 3000000, action_str : network\n",
      "running reward: 8.37 at episode 178454, frame count 3010000, action_str : random valid_action\n",
      "running reward: 7.78 at episode 179039, frame count 3020000, action_str : network\n",
      "running reward: 8.88 at episode 179631, frame count 3030000, action_str : network\n",
      "running reward: 8.01 at episode 180219, frame count 3040000, action_str : network\n",
      "running reward: 7.42 at episode 180811, frame count 3050000, action_str : network\n",
      "running reward: 7.72 at episode 181406, frame count 3060000, action_str : network\n",
      "running reward: 7.65 at episode 181996, frame count 3070000, action_str : network\n",
      "running reward: 7.26 at episode 182586, frame count 3080000, action_str : network\n",
      "running reward: 7.84 at episode 183179, frame count 3090000, action_str : random valid_action\n",
      "running reward: 8.35 at episode 183766, frame count 3100000, action_str : network\n",
      "running reward: 7.54 at episode 184355, frame count 3110000, action_str : network\n",
      "running reward: 8.80 at episode 184945, frame count 3120000, action_str : network\n",
      "running reward: 8.45 at episode 185538, frame count 3130000, action_str : network\n",
      "running reward: 9.79 at episode 186126, frame count 3140000, action_str : network\n",
      "running reward: 8.24 at episode 186727, frame count 3150000, action_str : network\n",
      "running reward: 9.37 at episode 187319, frame count 3160000, action_str : random valid_action\n",
      "running reward: 7.71 at episode 187911, frame count 3170000, action_str : random valid_action\n",
      "running reward: 8.33 at episode 188504, frame count 3180000, action_str : network\n",
      "running reward: 9.43 at episode 189096, frame count 3190000, action_str : network\n",
      "running reward: 9.07 at episode 189690, frame count 3200000, action_str : network\n",
      "running reward: 8.33 at episode 190277, frame count 3210000, action_str : network\n",
      "running reward: 7.31 at episode 190866, frame count 3220000, action_str : network\n",
      "running reward: 7.95 at episode 191455, frame count 3230000, action_str : network\n",
      "running reward: 9.87 at episode 192040, frame count 3240000, action_str : random valid_action\n",
      "running reward: 8.94 at episode 192628, frame count 3250000, action_str : network\n",
      "running reward: 9.79 at episode 193213, frame count 3260000, action_str : network\n",
      "running reward: 9.12 at episode 193796, frame count 3270000, action_str : network\n",
      "running reward: 10.05 at episode 194378, frame count 3280000, action_str : network\n",
      "running reward: 7.21 at episode 194961, frame count 3290000, action_str : network\n",
      "running reward: 8.86 at episode 195536, frame count 3300000, action_str : network\n",
      "running reward: 9.39 at episode 196125, frame count 3310000, action_str : network\n",
      "running reward: 8.50 at episode 196707, frame count 3320000, action_str : network\n",
      "running reward: 7.60 at episode 197290, frame count 3330000, action_str : random valid_action\n",
      "running reward: 8.21 at episode 197876, frame count 3340000, action_str : network\n"
     ]
    }
   ],
   "source": [
    "for episode_count in tqdm(range(max_episode)):\n",
    "\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "        if timestep == 1:\n",
    "            state = env.getGameState()\n",
    "            state = torch.tensor([[state]], dtype=torch.float, device=device)\n",
    "\n",
    "        frame_count = frame_count + 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        is_random_frames = frame_count < epsilon_random_frames\n",
    "        is_random_epsilon = epsilon > np.random.rand(1)[0]\n",
    "\n",
    "        if is_random_frames or is_random_epsilon:\n",
    "\n",
    "            # find useful move\n",
    "            valid_action_indexes, valid_actions = env.findNotDuplicateMove()\n",
    "            num_valid_actions = valid_actions.shape[0]\n",
    "            invalid_action_indexes = list(\n",
    "                set(range(num_actions)) - set(valid_action_indexes)\n",
    "            )\n",
    "\n",
    "            # selection useful move or do nothing\n",
    "            p_invalid = 1 / (num_valid_actions + 1)\n",
    "\n",
    "            if np.random.rand(1)[0] < p_invalid:\n",
    "                # do nothing\n",
    "                action = np.random.choice(invalid_action_indexes)\n",
    "                action_str = \"random invalid_action\"\n",
    "            else:\n",
    "                # move with valid move\n",
    "                action = np.random.choice(valid_action_indexes)\n",
    "                action_str = \"random valid_action\"\n",
    "\n",
    "            # convert to tensor\n",
    "            action = torch.tensor([[action]], device=device, dtype=torch.long)\n",
    "\n",
    "        else:\n",
    "            action_str = \"network\"\n",
    "\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            # Take best action\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon = epsilon - (epsilon_interval / epsilon_greedy_frames)\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        next_state, reward, is_gameover = env.move(action_list[action])\n",
    "        episode_reward = episode_reward + reward\n",
    "\n",
    "        # Convert to torch.tensor\n",
    "        reward = torch.tensor([reward], dtype=torch.float, device=device)\n",
    "        next_state = torch.tensor([[next_state]], dtype=torch.float, device=device)\n",
    "        if is_gameover:\n",
    "            next_state = None\n",
    "\n",
    "        # Save to replay buffer\n",
    "        memory.push(\n",
    "            state,\n",
    "            action,\n",
    "            reward,\n",
    "            next_state,\n",
    "        )\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Update action model\n",
    "        if frame_count % update_after_actions == 0 and len(memory) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            transitions = memory.sample(batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            non_final_mask = torch.tensor(\n",
    "                tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                device=device,\n",
    "                dtype=torch.bool,\n",
    "            )\n",
    "            non_final_next_states = torch.cat(\n",
    "                [s for s in batch.next_state if s is not None]\n",
    "            )\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t)\n",
    "            state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            next_state_values = torch.zeros(batch_size, device=device)\n",
    "            next_state_values[non_final_mask] = (\n",
    "                target_net(non_final_next_states).max(1)[0].detach()\n",
    "            )\n",
    "\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "            # If gameover set expected Q values = -1\n",
    "            expected_state_action_values[~non_final_mask] = -1\n",
    "\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = loss_function(\n",
    "                state_action_values, expected_state_action_values.unsqueeze(1)\n",
    "            )\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in policy_net.parameters():\n",
    "                param.grad.data = param.grad.data.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update target model\n",
    "        if frame_count % update_target_network == 0:\n",
    "\n",
    "            # Update the the target network with new weights\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            # Log details\n",
    "            template = (\n",
    "                \"running reward: {:.2f} at episode {}, frame count {}, action_str : {}\"\n",
    "            )\n",
    "\n",
    "            running_reward = np.mean(episode_reward_history)\n",
    "            running_reward_history.append(running_reward)\n",
    "\n",
    "            if len(running_reward_history) > 100:\n",
    "                running_reward_history = running_reward_history[1:]\n",
    "\n",
    "            print(\n",
    "                template.format(\n",
    "                    running_reward,\n",
    "                    episode_count,\n",
    "                    frame_count,\n",
    "                    action_str,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if is_gameover:\n",
    "            break\n",
    "\n",
    "        if is_break:\n",
    "            break\n",
    "\n",
    "    # Update running reward\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        episode_reward_history = episode_reward_history[1:]\n",
    "\n",
    "    if is_break:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbef80-5cb8-4e87-ae26-7d360cddd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"policy_net_move_select_1.pth\"\n",
    "torch.save(policy_net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c837e2-6c91-44ca-bd65-8fd76bbd4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(running_reward_history, label=f\"reward_history\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa4c73-4007-40cc-93d8-9117b486006d",
   "metadata": {},
   "source": [
    "# test play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aafe78-1fd1-421e-8886-590d75ebaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29197247-bc16-44d4-a48a-0c10e12af239",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "temp_board_history = []\n",
    "move_history = []\n",
    "score_history = []\n",
    "move_index = env.findAllMove()\n",
    "for i in range(1, max_steps_per_episode):\n",
    "    temp_board = env.getGameState()\n",
    "    state = torch.tensor([[temp_board]], dtype=torch.float, device=device)\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    move_history_str = (\n",
    "        f\"move {i+1} : move from {move_index[action,0,:]} to {move_index[action,1,:]}\"\n",
    "    )\n",
    "    _, round_score, is_gameover = env.move(move_index[action])\n",
    "    score_history.append(round_score)\n",
    "    temp_board_history.append(temp_board)\n",
    "    move_history.append(move_history_str)\n",
    "    if is_gameover:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e16f0a-9305-447f-8bb9-b0bcc536cb14",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeff62a-b77a-4d9c-a956-17e17e302ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    temp_board = temp_board_history[i]\n",
    "    ax.clear()\n",
    "    ax.pcolor(\n",
    "        temp_board,\n",
    "        cmap=env.box_color_map,\n",
    "        vmin=0,\n",
    "        vmax=env.unique_box_type,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    xticks_label = np.arange(0, env.board_width, 1)\n",
    "    yticks_label = np.arange(0, env.board_height + 1, 1)\n",
    "    # centering of call\n",
    "    ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "    ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "    # label\n",
    "    yticks_label = yticks_label.astype(np.str_)\n",
    "    yticks_label[-1] = \"top\"\n",
    "    ax.set_xticklabels(xticks_label)\n",
    "    ax.set_yticklabels(yticks_label)\n",
    "    ax.set_title(\n",
    "        move_history[i]\n",
    "        + f\"\\n total score : {np.sum(score_history[:i],dtype=int)}, round score : {score_history[i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598d98b-246c-4084-8c53-60d6e9dcef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.close(fig)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    animate,\n",
    "    frames=len(temp_board_history),\n",
    "    interval=500,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46915dc-161a-41f1-adbf-065aa04d7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4a9ee-40cb-4f10-82c4-ca3781d15f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ani = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfff555-4c89-485e-bbee-1b8ceb01dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation as an animated GIF\n",
    "if save_ani:\n",
    "    ani.save(\"policy_play_animation.gif\", dpi=300, writer=PillowWriter(fps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ed830-8429-4711-87fd-0614aafca799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
