{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415eab21-d051-4aeb-9d3f-a8de959c3e10",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322f5bd-529b-4296-8b30-af6cb379081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e944016-42ab-4686-9eed-1515c9c308cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872951e3-cc4e-4a81-bb5c-78e9313d69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28944f-5c87-4f1f-8aad-552f42f6841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da2d04-0a4b-425e-9a81-f4a7564954d5",
   "metadata": {},
   "source": [
    "# set pytorch gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef5c20-95eb-4e55-aa1a-fe915e528924",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacb66e-8d5d-45da-bf06-3f1ca56bdc40",
   "metadata": {},
   "source": [
    "# notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f6abe-8307-41b9-a16e-5388bca9bb21",
   "metadata": {},
   "source": [
    "## bit definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855463ac-827d-4cb6-a6f9-251011618e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228550ef-34c5-49b0-955d-df64bc655f57",
   "metadata": {},
   "source": [
    "| box_value | box_name     |\n",
    "|-----------|--------------|\n",
    "| 0         | Empty        |\n",
    "| 1         | BrownWood    |\n",
    "| 2         | RedWood      |\n",
    "| 3         | GreenWood    |\n",
    "| 4         | BrownLeather |\n",
    "| 5         | BrownPaper   |\n",
    "| 6         | BlueSteel    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19309f-30a4-4d7a-85fa-909dc4fe43a0",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bdddc-4d14-479e-ab88-a73b151b6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveTheBoxGameEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_height=9,\n",
    "        board_width=7,\n",
    "        unique_box_type=6,\n",
    "        p_add_3_box=0.77,\n",
    "        is_debug=False,\n",
    "    ):\n",
    "\n",
    "        # init parameter\n",
    "        self.board_height = board_height\n",
    "        self.board_width = board_width\n",
    "        self.unique_box_type = unique_box_type\n",
    "        self.p_add_3_box = p_add_3_box\n",
    "        self.is_debug = is_debug\n",
    "\n",
    "        # init color_map\n",
    "        box_color_map_list = (\n",
    "            np.array(\n",
    "                [\n",
    "                    [255, 255, 255],\n",
    "                    [169, 133, 92],\n",
    "                    [183, 71, 34],\n",
    "                    [156, 164, 52],\n",
    "                    [102, 65, 38],\n",
    "                    [189, 140, 61],\n",
    "                    [149, 165, 168],\n",
    "                ]\n",
    "            )\n",
    "            / 255\n",
    "        )\n",
    "        self.box_color_map = LinearSegmentedColormap.from_list(\n",
    "            \"box_color_map\",\n",
    "            box_color_map_list,\n",
    "            box_color_map_list.shape[0],\n",
    "        )\n",
    "\n",
    "        # init game\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_score = 0\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def createBoard(self):\n",
    "        self.board = np.zeros(\n",
    "            (self.board_height, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def createAboveBoard(self):\n",
    "        self.above_board = np.zeros(\n",
    "            (1, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def addAboveBox(self):\n",
    "        number_of_box_above = np.random.choice(\n",
    "            [3, 4], p=[self.p_add_3_box, 1 - self.p_add_3_box]\n",
    "        )\n",
    "        box_value = np.random.randint(\n",
    "            low=1,\n",
    "            high=self.unique_box_type + 1,\n",
    "            size=number_of_box_above,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox = np.zeros(\n",
    "            self.board_width,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox[:number_of_box_above] = 1\n",
    "        aboveBox[aboveBox == 1] = box_value\n",
    "        np.random.shuffle(aboveBox)\n",
    "        self.above_board = aboveBox.reshape((1, self.board_width))\n",
    "\n",
    "    def aboveIncludeFall(self):\n",
    "        is_gameover = self.checkIsGameover()\n",
    "        if is_gameover:\n",
    "            return is_gameover\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        self.above_board = temp_board[-1].reshape((1, self.board_width)).copy()\n",
    "        self.board = temp_board[:-1].copy()\n",
    "        self.addAboveBox()\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        if self.checkIsEmptyBoard():\n",
    "            return self.aboveIncludeFall()\n",
    "        return False\n",
    "\n",
    "    # game mechanism\n",
    "    def fall(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        for y in range(self.board_width):\n",
    "            x = 0\n",
    "            is_fall = False\n",
    "            while x < temp_board.shape[0] - 1:\n",
    "                # skip floor and run loop\n",
    "                x += 1\n",
    "                # check below is empty\n",
    "                if not self.checkIsEmpty(temp_board[x, y]) and self.checkIsEmpty(\n",
    "                    temp_board[x - 1, y]\n",
    "                ):\n",
    "                    # fall\n",
    "                    temp_board[x - 1, y] = temp_board[x, y].copy()\n",
    "                    temp_board[x, y] = 0\n",
    "                    is_fall = True\n",
    "\n",
    "                # reset and fall again\n",
    "                if is_fall and (x >= temp_board.shape[0] - 1):\n",
    "                    x = 0\n",
    "                    is_fall = False\n",
    "        return temp_board\n",
    "\n",
    "    def remove(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        if self.is_debug:\n",
    "            self.displayTempBoard(temp_board)\n",
    "        remove_list = self.check(temp_board)\n",
    "        numbers_of_remove = np.unique(remove_list, axis=0).shape[0]\n",
    "        if self.is_debug:\n",
    "            print(f\"remove_list : {remove_list}\")\n",
    "            print(f\"number of remove box (remove): {numbers_of_remove}\")\n",
    "        numbers_of_remove_recursive = []\n",
    "        if len(remove_list) > 0:\n",
    "            for remove_index in remove_list:\n",
    "                temp_board[remove_index[0], remove_index[1]] = 0\n",
    "            temp_board = self.fall(temp_board)\n",
    "            temp_board, numbers_of_remove_recursive = self.remove(temp_board)\n",
    "        return temp_board, [numbers_of_remove] + numbers_of_remove_recursive\n",
    "\n",
    "    def check(self, temp_board):\n",
    "        remove_list = []\n",
    "        for x in range(temp_board.shape[0]):\n",
    "            remove_list = remove_list + self.checkRow(temp_board, x)\n",
    "\n",
    "        for y in range(temp_board.shape[1]):\n",
    "            remove_list = remove_list + self.checkCol(temp_board, y)\n",
    "        return remove_list\n",
    "\n",
    "    def checkRow(self, temp_board, x):\n",
    "        count = 1\n",
    "        last_found = temp_board[x, 0]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "\n",
    "        for y in range(1, temp_board.shape[1]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if y >= temp_board.shape[1] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x, y - 1]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x, y - i - 1])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def checkCol(self, temp_board, y):\n",
    "        count = 1\n",
    "        last_found = temp_board[0, y]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "        for x in range(1, temp_board.shape[0]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if x >= temp_board.shape[0] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x - 1, y]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x - i - 1, y])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def updateBoard(self):\n",
    "        temp_board = self.board.copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        temp_board, numbers_of_remove = self.remove(temp_board)\n",
    "        self.board = temp_board.copy()\n",
    "        if self.is_debug:\n",
    "            print(f\"numbers_of_remove (updateBoard): {numbers_of_remove}\")\n",
    "        return self.calculatePoint(numbers_of_remove)\n",
    "\n",
    "    def calculatePoint(self, numbers_of_remove_box):\n",
    "        numbers_of_remove_box = np.array(numbers_of_remove_box)\n",
    "        points = []\n",
    "        for t, number_of_box in enumerate(numbers_of_remove_box):\n",
    "            if number_of_box > 0:\n",
    "                summation = 0\n",
    "                for i in range(0, number_of_box - 3 + 1):\n",
    "                    summation = summation + i\n",
    "                if t == 0:\n",
    "                    point = number_of_box + summation\n",
    "                else:\n",
    "                    main_point = numbers_of_remove_box[t] * np.sum(\n",
    "                        numbers_of_remove_box[:t]\n",
    "                    )\n",
    "                    point = main_point + (number_of_box - 3) + summation\n",
    "            else:\n",
    "                point = 0\n",
    "            points.append(point)\n",
    "        return np.sum(points)\n",
    "\n",
    "    # utility\n",
    "    def displayBoard(self):\n",
    "        # create virtual board\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height + 1, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        yticks_label = yticks_label.astype(np.str)\n",
    "        yticks_label[-1] = \"above\"\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def displayTempBoard(self, temp_board):\n",
    "        # create virtual board\n",
    "        temp_board = temp_board.copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def checkIsEmpty(self, box):\n",
    "        return box == 0\n",
    "\n",
    "    def checkIsSameColor(self, box1, box2):\n",
    "        return box1 == box2\n",
    "\n",
    "    def checkIsEmptyBoard(self):\n",
    "        return (self.board == 0).all()\n",
    "\n",
    "    # action\n",
    "    def findAllMove(self):\n",
    "        all_move_index = []\n",
    "        row_arange = np.arange(0, self.board_height, 1)\n",
    "        col_arange = np.arange(0, self.board_width, 1)\n",
    "        row_index, col_index = np.meshgrid(row_arange, col_arange)\n",
    "        point_index = np.vstack((row_index.reshape(-1), col_index.reshape(-1))).T\n",
    "\n",
    "        for index in point_index:\n",
    "\n",
    "            if index[1] < self.board_width - 1:\n",
    "                # move_right\n",
    "                move_right_index = np.array([index, index + np.array([0, 1])])\n",
    "                all_move_index.append(move_right_index)\n",
    "\n",
    "            if index[0] < self.board_height - 1:\n",
    "                # move_up\n",
    "                move_up_index = np.array([index, index + np.array([1, 0])])\n",
    "                all_move_index.append(move_up_index)\n",
    "\n",
    "        # add_do_noting\n",
    "        all_move_index.append(np.array([[0, 0], [0, 0]]))\n",
    "        return np.array(all_move_index)\n",
    "\n",
    "    def findNotDuplicateMove(self):\n",
    "        all_move_index = self.findAllMove()\n",
    "\n",
    "        not_duplicate_move_index = []\n",
    "        not_duplicate_array_index = []\n",
    "        for i, move_index in enumerate(all_move_index):\n",
    "            point_a_index = move_index[0]\n",
    "            point_a_value = self.board[point_a_index[0], point_a_index[1]]\n",
    "\n",
    "            point_b_index = move_index[1]\n",
    "            point_b_value = self.board[point_b_index[0], point_b_index[1]]\n",
    "\n",
    "            if not self.checkIsSameColor(point_a_value, point_b_value):\n",
    "                # not move up with empty\n",
    "                if not (\n",
    "                    (point_a_index[1] == point_b_index[1])\n",
    "                    and (\n",
    "                        self.checkIsEmpty(point_a_value)\n",
    "                        or self.checkIsEmpty(point_b_value)\n",
    "                    )\n",
    "                ):\n",
    "                    not_duplicate_move_index.append(move_index)\n",
    "                    not_duplicate_array_index.append(i)\n",
    "\n",
    "        not_duplicate_move_index.append(np.array([[0, 0], [0, 0]]))\n",
    "        not_duplicate_array_index.append(len(not_duplicate_move_index) - 1)\n",
    "\n",
    "        return np.array(not_duplicate_array_index), np.array(not_duplicate_move_index)\n",
    "\n",
    "    def move(self, move_index):\n",
    "        before_cumulative_score = self.cumulative_score\n",
    "        # collect value\n",
    "        point_a_index = move_index[0]\n",
    "        point_a_value = self.board[point_a_index[0], point_a_index[1]].copy()\n",
    "\n",
    "        point_b_index = move_index[1]\n",
    "        point_b_value = self.board[point_b_index[0], point_b_index[1]].copy()\n",
    "\n",
    "        # move\n",
    "        self.board[point_a_index[0], point_a_index[1]] = point_b_value\n",
    "        self.board[point_b_index[0], point_b_index[1]] = point_a_value\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        is_gameover = self.aboveIncludeFall()\n",
    "        return (\n",
    "            self.getGameState(),\n",
    "            self.cumulative_score - before_cumulative_score,\n",
    "            is_gameover,\n",
    "        )\n",
    "\n",
    "    def checkIsGameover(self):\n",
    "        for y in range(self.board.shape[1]):\n",
    "            full_board = (self.board[:, y] != 0).all()\n",
    "            full_above_board = (self.above_board[:, y] != 0).all()\n",
    "            if full_board and full_above_board:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Reinforcement\n",
    "    def getGameState(self):\n",
    "        return np.vstack((self.board, self.above_board)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6b3e6-7d5d-4a1b-83b6-2faa55778f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(test_env, max_steps_per_episode=1000):\n",
    "\n",
    "    test_env.reset()\n",
    "    temp_board_history = []\n",
    "    move_history = []\n",
    "    score_history = []\n",
    "    for i in range(max_steps_per_episode):\n",
    "        temp_board = test_env.getGameState()\n",
    "        _, move_index = test_env.findNotDuplicateMove()\n",
    "        select_move_index = np.random.randint(move_index.shape[0])\n",
    "        move_history_str = f\"move {i+1} : move from {move_index[select_move_index,0,:]} to {move_index[select_move_index,1,:]}\"\n",
    "        _, round_score, is_gameover = test_env.move(move_index[select_move_index])\n",
    "        score_history.append(round_score)\n",
    "        temp_board_history.append(temp_board)\n",
    "        move_history.append(move_history_str)\n",
    "        if is_gameover:\n",
    "            break\n",
    "    return test_env.cumulative_score, temp_board_history, move_history, score_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5bb93-3c57-4df2-be48-e8bc9797c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_play(policy_net, test_env, max_steps_per_episode=1000):\n",
    "\n",
    "    test_env.reset()\n",
    "    temp_board_history = []\n",
    "    move_history = []\n",
    "    score_history = []\n",
    "    move_index = test_env.findAllMove()\n",
    "\n",
    "    for i in range(1, max_steps_per_episode):\n",
    "        temp_board = test_env.getGameState()\n",
    "        state = torch.tensor([[temp_board]], dtype=torch.float, device=device)\n",
    "        with torch.no_grad():\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "        move_history_str = f\"move {i+1} : move from {move_index[action,0,:]} to {move_index[action,1,:]}\"\n",
    "        _, round_score, is_gameover = test_env.move(move_index[action])\n",
    "\n",
    "        score_history.append(round_score)\n",
    "        temp_board_history.append(temp_board)\n",
    "        move_history.append(move_history_str)\n",
    "\n",
    "        if is_gameover:\n",
    "            break\n",
    "\n",
    "    return test_env.cumulative_score, temp_board_history, move_history, score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f9404-971f-446e-9be7-d8a19f946586",
   "metadata": {},
   "source": [
    "# deep q learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e23c4-ca1c-4b8e-ad13-6b0e3df81bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.linear1 = nn.Linear(576, 64)\n",
    "        self.linear2 = nn.Linear(64, 16)\n",
    "        self.linear3 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb949e0b-1edc-4e2b-a110-f76b779bb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1090e-3efc-4a1f-939a-e512181199eb",
   "metadata": {},
   "source": [
    "# replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fa488-f82f-4330-86c4-0199b04e9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\n",
    "        \"state\",\n",
    "        \"action\",\n",
    "        \"reward\",\n",
    "        \"next_state\",\n",
    "        \"is_invalid_action\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a7698-40b0-412f-8d48-9d7a2389471d",
   "metadata": {},
   "source": [
    "# utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be58bda-1c6d-46cf-9d50-2bb4a238fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_animate(i):\n",
    "    temp_board = temp_board_history[i]\n",
    "    ax.clear()\n",
    "    ax.pcolor(\n",
    "        temp_board,\n",
    "        cmap=env.box_color_map,\n",
    "        vmin=0,\n",
    "        vmax=env.unique_box_type,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    xticks_label = np.arange(0, env.board_width, 1)\n",
    "    yticks_label = np.arange(0, env.board_height + 1, 1)\n",
    "    # centering of call\n",
    "    ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "    ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "    # label\n",
    "    yticks_label = yticks_label.astype(np.str_)\n",
    "    yticks_label[-1] = \"top\"\n",
    "    ax.set_xticklabels(xticks_label)\n",
    "    ax.set_yticklabels(yticks_label)\n",
    "    ax.set_title(\n",
    "        move_history[i]\n",
    "        + f\"\\n total score : {np.sum(score_history[:i],dtype=int)}, round score : {score_history[i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b901e-8c3d-4cf0-ae71-53d1fabe7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentiaMoveingAverageBias(time_serie, beta):\n",
    "    ema_array = []\n",
    "    ema = 0\n",
    "    for t in range(time_serie.shape[0]):\n",
    "        ema = (1 - beta) * ema + (beta) * time_serie[t]\n",
    "        ema_bias = ema / (1 - ((1 - beta) ** (t + 1)))\n",
    "        ema_array.append(ema_bias)\n",
    "    return ema_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd533a6-bdc8-4ae2-8265-67f29f4d3301",
   "metadata": {},
   "source": [
    "# setup parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede39068-b92a-4dc2-8da3-3bdf98fbbcc0",
   "metadata": {},
   "source": [
    "## model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f0092-7bb6-4f8f-857f-904df283e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quick running\n",
    "reduce_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195774e-1bc2-4951-b069-2835098756f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning parameter\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 1000\n",
    "max_episode = 1000000 // reduce_step\n",
    "max_buffer_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91772d-8dea-4c9b-b9f7-ef0cd495a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy parameter\n",
    "epsilon = 1.0\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "\n",
    "# epsilon greedy frame\n",
    "epsilon_random_frames = 2000000 // reduce_step\n",
    "epsilon_greedy_frames = 10000000 // reduce_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0071be8-e935-4134-b716-24bb2d517b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning parameter\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664dcb7-948e-4579-9c90-d438c393a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning parameter\n",
    "update_after_actions = 1\n",
    "update_target_network = np.minimum(10000, int(10000 / (reduce_step / 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b75ea7-d78b-414b-b066-c4d3784b3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation parameter\n",
    "num_evaluation_step = 100\n",
    "test_play_episode_after = max_episode // num_evaluation_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8f840-ad0f-4044-9306-94a6dfaed247",
   "metadata": {},
   "source": [
    "## save and load parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d83d7-848a-4e3e-8aa0-e5b590ab13fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_load_model = True\n",
    "load_model_path = \"policy_net_move_select.pth\"\n",
    "is_save_model = True\n",
    "save_model_path = \"policy_net_move_select1.pth\"\n",
    "is_save_policy_ani = True\n",
    "policy_ani_path = \"policy_play_animation.gif\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf22006-bcf4-4ebd-bbfd-15d880334f03",
   "metadata": {},
   "source": [
    "## dubug parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c6a02-4e4f-47b5-9bb4-70b3663413e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_run_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2f359-43c1-4330-b350-4f668d8c69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_debug = False\n",
    "is_break = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c017d74-bf19-41ef-aa7a-0413ae008af8",
   "metadata": {},
   "source": [
    "# create object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa24e27-a1c0-4242-b8c1-61a696eb790c",
   "metadata": {},
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791e5d8-9555-40fa-9a0b-d6c2ebfbc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b65b7d-faac-4368-a6a7-1f0c1ac6a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = env.findAllMove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0925f8-48fa-4bb9-81e8-8ffdf4c03c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_list.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0037b2-1c97-4969-8ccf-c46f5260446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_height = env.board_height\n",
    "board_width = env.board_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf4ecd-0895-42b2-8360-e938a10a6951",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cceada-6135-46ee-b030-2a8c6f0b2e0f",
   "metadata": {},
   "source": [
    "### policy net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa9615-966c-4644-a372-5ccb697db55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(board_height, board_width, num_actions).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05192cfb-c83f-48e5-ab09-0dad610bc127",
   "metadata": {},
   "source": [
    "#### load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5f251-e011-406e-8e46-4da0e25bc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_load_model:\n",
    "    policy_net.load_state_dict(torch.load(load_model_path))\n",
    "else:\n",
    "    policy_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389a258-6ca8-4396-bb8b-e8b6a2d105b1",
   "metadata": {},
   "source": [
    "### target net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfefb3f-a9ea-4c9a-b68c-2fdc1c0748ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net = DQN(board_height, board_width, num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a648321-5357-417e-beca-9f0ec3824368",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80138c9e-c458-4ced-b3ba-598b96799c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    policy_net.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15e6b0-a271-45f2-9be6-c3b35d1598c5",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f16369-906a-4f35-877d-901f4afb1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9908943-45be-4720-91ed-f12d7564ab99",
   "metadata": {},
   "source": [
    "## replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91f693-290e-4336-8eca-0ea09f3cd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(max_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ece3b-b222-4164-8711-8dc3b1225a97",
   "metadata": {},
   "source": [
    "## episode replay reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7b14b-1ecb-4bc5-89f3-33cce99693fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_play_episode_reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770fac17-8980-4baa-9bcc-aa36debc639d",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524613e-0d0c-42ee-ae8e-32099b80c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065ba99-ddf5-44a5-8f6d-bdcebe3f82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_state_action_values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57579bca-874d-4352-ba69-90535ad03f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_run_model:\n",
    "    for episode_count in tqdm(range(max_episode)):\n",
    "\n",
    "        env.reset()\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "            if timestep == 1:\n",
    "                state = env.getGameState()\n",
    "                state = torch.tensor([[state]], dtype=torch.float, device=device)\n",
    "\n",
    "            frame_count = frame_count + 1\n",
    "\n",
    "            # Use epsilon-greedy for exploration\n",
    "            is_random_frames = frame_count < epsilon_random_frames\n",
    "            is_random_epsilon = epsilon > np.random.rand(1)[0]\n",
    "\n",
    "            # find useful move\n",
    "            valid_action_indexes, valid_actions = env.findNotDuplicateMove()\n",
    "\n",
    "            is_invalid_action = False\n",
    "\n",
    "            if is_random_frames or is_random_epsilon:\n",
    "\n",
    "                # selection useful move or do nothing\n",
    "                action = np.random.choice(valid_action_indexes)\n",
    "                action_str = \"random valid_action\"\n",
    "\n",
    "                # convert to tensor\n",
    "                action = torch.tensor([[action]], dtype=torch.long, device=device)\n",
    "\n",
    "            else:\n",
    "                action_str = \"network valid_action\"\n",
    "\n",
    "                # Predict action Q-values\n",
    "                # From environment state\n",
    "                # Take best action\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
    "\n",
    "                    if action.item() not in valid_action_indexes:\n",
    "                        is_invalid_action = True\n",
    "                        action_str = \"network invalid_action\"\n",
    "\n",
    "            # Decay probability of taking random action\n",
    "            epsilon = epsilon - (epsilon_interval / epsilon_greedy_frames)\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            next_state, reward, is_gameover = env.move(action_list[action])\n",
    "\n",
    "            # Convert to torch.tensor\n",
    "            reward = torch.tensor([reward], dtype=torch.float, device=device)\n",
    "            next_state = torch.tensor([[next_state]], dtype=torch.float, device=device)\n",
    "            is_invalid_action = torch.tensor(\n",
    "                [is_invalid_action], dtype=torch.bool, device=device\n",
    "            )\n",
    "\n",
    "            if is_gameover:\n",
    "                next_state = None\n",
    "\n",
    "            # Save to replay buffer\n",
    "            memory.push(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state,\n",
    "                is_invalid_action,\n",
    "            )\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # Update action model\n",
    "            if frame_count % update_after_actions == 0 and len(memory) > batch_size:\n",
    "\n",
    "                # Get indices of samples for replay buffers\n",
    "                transitions = memory.sample(batch_size)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "\n",
    "                # Using list comprehension to sample from replay buffer\n",
    "                non_final_mask = torch.tensor(\n",
    "                    tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                    device=device,\n",
    "                    dtype=torch.bool,\n",
    "                )\n",
    "\n",
    "                non_final_next_states = torch.cat(\n",
    "                    [s for s in batch.next_state if s is not None]\n",
    "                )\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.cat(batch.reward)\n",
    "                is_invalid_action = torch.cat(batch.is_invalid_action)\n",
    "\n",
    "                # Compute Q(s_t, a) - the model computes Q(s_t)\n",
    "                state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "                # Compute V(s_{t+1}) for all next states.\n",
    "                next_state_values = torch.zeros(batch_size, device=device)\n",
    "                next_state_values[non_final_mask] = (\n",
    "                    target_net(non_final_next_states).max(1)[0].detach()\n",
    "                )\n",
    "\n",
    "                # Compute the expected Q values\n",
    "                expected_state_action_values = (\n",
    "                    next_state_values * gamma\n",
    "                ) + reward_batch\n",
    "\n",
    "                # If gameover set expected Q values = -1\n",
    "                expected_state_action_values[~non_final_mask] = -1\n",
    "\n",
    "                # If useless move set expected Q values = -1\n",
    "                expected_state_action_values[is_invalid_action] = -1\n",
    "\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(\n",
    "                    state_action_values, expected_state_action_values.unsqueeze(1)\n",
    "                )\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                for param in policy_net.parameters():\n",
    "                    param.grad.data = param.grad.data.clamp_(-1, 1)\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target model\n",
    "            if frame_count % update_target_network == 0:\n",
    "                # Update the the target network with new weights\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if is_gameover:\n",
    "                break\n",
    "\n",
    "            if is_break:\n",
    "                break\n",
    "\n",
    "        # Update running reward\n",
    "        if episode_count % test_play_episode_after == 0:\n",
    "            test_env = MoveTheBoxGameEnvironment()\n",
    "            test_play_episode_reward, _, _, _ = policy_play(policy_net, test_env)\n",
    "            test_play_episode_reward_history.append(test_play_episode_reward)\n",
    "\n",
    "            template = (\n",
    "                \"test number {}, episode reward: {:.2f}, at episode {}, frame count {}\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                template.format(\n",
    "                    episode_count // test_play_episode_after,\n",
    "                    test_play_episode_reward,\n",
    "                    episode_count,\n",
    "                    frame_count,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if is_break:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49dfdf-b76f-475e-8eec-0e4f5933bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_save_model:\n",
    "    torch.save(policy_net.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd17466-6127-452e-940d-2d7adf5bffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_run_model:\n",
    "    fig = plt.figure(figsize=(7, 4.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    test_play_episode_reward_history = np.array(test_play_episode_reward_history)\n",
    "\n",
    "    ax.plot(\n",
    "        np.linspace(0, max_episode, len(test_play_episode_reward_history)),\n",
    "        test_play_episode_reward_history,\n",
    "        label=f\"reward_history\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        np.linspace(0, max_episode, len(test_play_episode_reward_history)),\n",
    "        exponentiaMoveingAverageBias(test_play_episode_reward_history, 0.5),\n",
    "        label=f\"ema .5 reward_history\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        np.linspace(0, max_episode, len(test_play_episode_reward_history)),\n",
    "        exponentiaMoveingAverageBias(test_play_episode_reward_history, 0.1),\n",
    "        label=f\"ema .1 reward_history\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"episode\")\n",
    "    ax.set_ylabel(\"reward\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192fecd-1b4b-401f-9299-9075d4f6c4a2",
   "metadata": {},
   "source": [
    "# test random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596f621-aaba-43e1-9965-cf6e8da1079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8cf37-7508-4a0f-9828-0e08a1609635",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06335fc5-27fc-4faf-bc8c-784f8e487499",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, temp_board_history, move_history, score_history = random_play(test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffb07c-2ef7-4d93-bf5c-b9a26294df66",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80ce21-9f98-45c1-a953-ca6df7d557df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.close(fig)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    replay_animate,\n",
    "    frames=len(temp_board_history),\n",
    "    interval=500,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac8e0f-a882-4ee8-a252-f9704e414043",
   "metadata": {},
   "source": [
    "# test policy play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc66d16-80c8-4740-86a9-ee265fdd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972138e3-6f16-4c9c-b24e-e42721e5a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, temp_board_history, move_history, score_history = policy_play(policy_net, test_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267bb96-d154-4078-a07c-f5738f36a837",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723030a8-6247-4aba-a093-74766570a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.close(fig)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    replay_animate,\n",
    "    frames=len(temp_board_history),\n",
    "    interval=500,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f0d008-9690-4b9c-aa81-543a107b3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_save_policy_ani:\n",
    "    from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cb1ff-0564-49a9-b7fe-d1d30f57b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation as an animated GIF\n",
    "if is_save_policy_ani:\n",
    "    ani.save(policy_ani_path, dpi=300, writer=PillowWriter(fps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273a3b8-27fb-4a61-bcfa-1ebcb9a82526",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca817475-e3d9-4925-bdc6-287de92e05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = test_env.getGameState()\n",
    "state = torch.tensor([[state]], dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d43e85-b502-4f46-a776-24918bf1f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aacab3-0382-41ef-a610-f4f7f049eae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
