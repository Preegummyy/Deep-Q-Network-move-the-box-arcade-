{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e2e6e7-2718-43fa-8188-48adf6ce236e",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a25cb773-92ba-4285-b1c7-0c2731899d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b8e4d4-36b9-4c0a-b94f-3292faa95694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ef4c9e-1373-4526-9649-e498005d05ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648e6b3f-a048-4280-983c-f16389e4052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade7a96-df3f-4309-a35e-15535a2c7b65",
   "metadata": {},
   "source": [
    "# set tensorflow gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a669c5-1959-40a9-864d-ff2984deae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587cb524-a3e4-47b8-bda5-16f65dbd17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type = [device.device_type for device in tf.config.list_physical_devices()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3494c4-b7f7-4e64-ab6a-673bd8a21e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# if has GPU run on GPU first\n",
    "device_name = \"/device:GPU:0\" if \"GPU\" in device_type else \"/device:CPU:0\"\n",
    "print(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04087884-2da8-4406-8e78-e12d10c91d8c",
   "metadata": {},
   "source": [
    "# notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694fa66-470b-40f0-bc88-8543958705e6",
   "metadata": {},
   "source": [
    "## bit definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1180af2a-10d2-4780-8d85-0421b378eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49772e-48b6-42d4-9a91-fea77ea9ad69",
   "metadata": {},
   "source": [
    "| box_value | box_name     |\n",
    "|-----------|--------------|\n",
    "| 0         | Empty        |\n",
    "| 1         | BrownWood    |\n",
    "| 2         | RedWood      |\n",
    "| 3         | GreenWood    |\n",
    "| 4         | BrownLeather |\n",
    "| 5         | BrownPaper   |\n",
    "| 6         | BlueSteel    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de80c4f-0f49-4f94-a3bc-39c93cedc33c",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50331f6-5161-483e-8c15-1552f76ec948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveTheBoxGameEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_height=9,\n",
    "        board_width=7,\n",
    "        unique_box_type=6,\n",
    "        p_add_3_box=0.77,\n",
    "        debug=False,\n",
    "    ):\n",
    "\n",
    "        # init parameter\n",
    "        self.board_height = board_height\n",
    "        self.board_width = board_width\n",
    "        self.unique_box_type = unique_box_type\n",
    "        self.p_add_3_box = p_add_3_box\n",
    "        self.debug = debug\n",
    "        self.cumulative_score = 0\n",
    "\n",
    "        # init color_map\n",
    "        box_color_map_list = (\n",
    "            np.array(\n",
    "                [\n",
    "                    [255, 255, 255],\n",
    "                    [169, 133, 92],\n",
    "                    [183, 71, 34],\n",
    "                    [156, 164, 52],\n",
    "                    [102, 65, 38],\n",
    "                    [189, 140, 61],\n",
    "                    [149, 165, 168],\n",
    "                ]\n",
    "            )\n",
    "            / 255\n",
    "        )\n",
    "        self.box_color_map = LinearSegmentedColormap.from_list(\n",
    "            \"box_color_map\",\n",
    "            box_color_map_list,\n",
    "            box_color_map_list.shape[0],\n",
    "        )\n",
    "\n",
    "        # init game\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def reset(self):\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def createBoard(self):\n",
    "        self.board = np.zeros(\n",
    "            (self.board_height, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def createAboveBoard(self):\n",
    "        self.above_board = np.zeros(\n",
    "            (1, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def addAboveBox(self):\n",
    "        number_of_box_above = np.random.choice(\n",
    "            [3, 4], p=[self.p_add_3_box, 1 - self.p_add_3_box]\n",
    "        )\n",
    "        box_value = np.random.randint(\n",
    "            low=1,\n",
    "            high=self.unique_box_type + 1,\n",
    "            size=number_of_box_above,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox = np.zeros(\n",
    "            self.board_width,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox[:number_of_box_above] = 1\n",
    "        aboveBox[aboveBox == 1] = box_value\n",
    "        np.random.shuffle(aboveBox)\n",
    "        self.above_board = aboveBox.reshape((1, self.board_width))\n",
    "\n",
    "    def aboveIncludeFall(self):\n",
    "        is_gameover = self.checkIsGameover()\n",
    "        if is_gameover:\n",
    "            return is_gameover\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        self.above_board = temp_board[-1].reshape((1, self.board_width)).copy()\n",
    "        self.board = temp_board[:-1].copy()\n",
    "        self.addAboveBox()\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        if self.checkIsEmptyBoard():\n",
    "            return self.aboveIncludeFall()\n",
    "        return False\n",
    "\n",
    "    # game mechanism\n",
    "    def fall(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        for y in range(self.board_width):\n",
    "            x = 0\n",
    "            is_fall = False\n",
    "            while x < temp_board.shape[0] - 1:\n",
    "                # skip floor and run loop\n",
    "                x += 1\n",
    "                # check below is empty\n",
    "                if not self.checkIsEmpty(temp_board[x, y]) and self.checkIsEmpty(\n",
    "                    temp_board[x - 1, y]\n",
    "                ):\n",
    "                    # fall\n",
    "                    temp_board[x - 1, y] = temp_board[x, y].copy()\n",
    "                    temp_board[x, y] = 0\n",
    "                    is_fall = True\n",
    "\n",
    "                # reset and fall again\n",
    "                if is_fall and (x >= temp_board.shape[0] - 1):\n",
    "                    x = 0\n",
    "                    is_fall = False\n",
    "        return temp_board\n",
    "\n",
    "    def remove(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            self.displayTempBoard(temp_board)\n",
    "        remove_list = self.check(temp_board)\n",
    "        numbers_of_remove = np.unique(remove_list, axis=0).shape[0]\n",
    "        if self.debug:\n",
    "            print(f\"remove_list : {remove_list}\")\n",
    "            print(f\"number of remove box (remove): {numbers_of_remove}\")\n",
    "        numbers_of_remove_recursive = []\n",
    "        if len(remove_list) > 0:\n",
    "            for remove_index in remove_list:\n",
    "                temp_board[remove_index[0], remove_index[1]] = 0\n",
    "            temp_board = self.fall(temp_board)\n",
    "            temp_board, numbers_of_remove_recursive = self.remove(temp_board)\n",
    "        return temp_board, [numbers_of_remove] + numbers_of_remove_recursive\n",
    "\n",
    "    def check(self, temp_board):\n",
    "        remove_list = []\n",
    "        for x in range(temp_board.shape[0]):\n",
    "            remove_list = remove_list + self.checkRow(temp_board, x)\n",
    "\n",
    "        for y in range(temp_board.shape[1]):\n",
    "            remove_list = remove_list + self.checkCol(temp_board, y)\n",
    "        return remove_list\n",
    "\n",
    "    def checkRow(self, temp_board, x):\n",
    "        count = 1\n",
    "        last_found = temp_board[x, 0]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "\n",
    "        for y in range(1, temp_board.shape[1]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if y >= temp_board.shape[1] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x, y - 1]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x, y - i - 1])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def checkCol(self, temp_board, y):\n",
    "        count = 1\n",
    "        last_found = temp_board[0, y]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "        for x in range(1, temp_board.shape[0]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if x >= temp_board.shape[0] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x - 1, y]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x - i - 1, y])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def updateBoard(self):\n",
    "        temp_board = self.board.copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        temp_board, numbers_of_remove = self.remove(temp_board)\n",
    "        self.board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            print(f\"numbers_of_remove (updateBoard): {numbers_of_remove}\")\n",
    "        return self.calculatePoint(numbers_of_remove)\n",
    "\n",
    "    def calculatePoint(self, numbers_of_remove_box):\n",
    "        numbers_of_remove_box = np.array(numbers_of_remove_box)\n",
    "        points = []\n",
    "        for t, number_of_box in enumerate(numbers_of_remove_box):\n",
    "            if number_of_box > 0:\n",
    "                summation = 0\n",
    "                for i in range(0, number_of_box - 3 + 1):\n",
    "                    summation = summation + i\n",
    "                if t == 0:\n",
    "                    point = number_of_box + summation\n",
    "                else:\n",
    "                    main_point = numbers_of_remove_box[t] * np.sum(\n",
    "                        numbers_of_remove_box[:t]\n",
    "                    )\n",
    "                    point = main_point + (number_of_box - 3) + summation\n",
    "            else:\n",
    "                point = 0\n",
    "            points.append(point)\n",
    "        return np.sum(points)\n",
    "\n",
    "    # utility\n",
    "    def displayBoard(self):\n",
    "        # create virtual board\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height + 1, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        yticks_label = yticks_label.astype(np.str)\n",
    "        yticks_label[-1] = \"above\"\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def displayTempBoard(self, temp_board):\n",
    "        # create virtual board\n",
    "        temp_board = temp_board.copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def checkIsEmpty(self, box):\n",
    "        return box == 0\n",
    "\n",
    "    def checkIsSameColor(self, box1, box2):\n",
    "        return box1 == box2\n",
    "\n",
    "    def checkIsEmptyBoard(self):\n",
    "        return (self.board == 0).all()\n",
    "\n",
    "    # action\n",
    "    def findAllMove(self):\n",
    "        all_move_index = []\n",
    "        row_arange = np.arange(0, self.board_height, 1)\n",
    "        col_arange = np.arange(0, self.board_width, 1)\n",
    "        row_index, col_index = np.meshgrid(row_arange, col_arange)\n",
    "        point_index = np.vstack((row_index.reshape(-1), col_index.reshape(-1))).T\n",
    "\n",
    "        for index in point_index:\n",
    "\n",
    "            if index[1] < self.board_width - 1:\n",
    "                # move_right\n",
    "                move_right_index = np.array([index, index + np.array([0, 1])])\n",
    "                all_move_index.append(move_right_index)\n",
    "\n",
    "            if index[0] < self.board_height - 1:\n",
    "                # move_up\n",
    "                move_up_index = np.array([index, index + np.array([1, 0])])\n",
    "                all_move_index.append(move_up_index)\n",
    "\n",
    "        return np.array(all_move_index)\n",
    "\n",
    "    def findNotDuplicateMove(self):\n",
    "        all_move_index = self.findAllMove()\n",
    "\n",
    "        not_duplicate_move_index = []\n",
    "        for move_index in all_move_index:\n",
    "            point_a_index = move_index[0]\n",
    "            point_a_value = self.board[point_a_index[0], point_a_index[1]]\n",
    "\n",
    "            point_b_index = move_index[1]\n",
    "            point_b_value = self.board[point_b_index[0], point_b_index[1]]\n",
    "\n",
    "            if not self.checkIsSameColor(point_a_value, point_b_value):\n",
    "                not_duplicate_move_index.append(move_index)\n",
    "\n",
    "        return np.array(not_duplicate_move_index)\n",
    "\n",
    "    def move(self, move_index):\n",
    "        before_cumulative_score = self.cumulative_score\n",
    "        # collect value\n",
    "        point_a_index = move_index[0]\n",
    "        point_a_value = self.board[point_a_index[0], point_a_index[1]].copy()\n",
    "\n",
    "        point_b_index = move_index[1]\n",
    "        point_b_value = self.board[point_b_index[0], point_b_index[1]].copy()\n",
    "\n",
    "        # move\n",
    "        self.board[point_a_index[0], point_a_index[1]] = point_b_value\n",
    "        self.board[point_b_index[0], point_b_index[1]] = point_a_value\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        is_gameover = self.aboveIncludeFall()\n",
    "        return (\n",
    "            self.getGameState(),\n",
    "            self.cumulative_score - before_cumulative_score,\n",
    "            is_gameover,\n",
    "        )\n",
    "\n",
    "    def checkIsGameover(self):\n",
    "        for y in range(self.board.shape[1]):\n",
    "            full_board = (self.board[:, y] != 0).all()\n",
    "            full_above_board = (self.above_board[:, y] != 0).all()\n",
    "            if full_board and full_above_board:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Reinforcement\n",
    "    def getGameState(self):\n",
    "        return np.vstack((self.board, self.above_board)).copy()\n",
    "\n",
    "    def play(self):\n",
    "        temp_board_history = []\n",
    "        move_history = []\n",
    "        score_history = []\n",
    "        for i in range(200):\n",
    "            temp_board = self.getGameState()\n",
    "            move_index = self.findNotDuplicateMove()\n",
    "            select_move_index = np.random.randint(move_index.shape[0])\n",
    "            move_history_str = f\"move {i+1} : move from {move_index[select_move_index,0,:]} to {move_index[select_move_index,1,:]}\"\n",
    "            _, round_score, is_gameover = self.move(move_index[select_move_index])\n",
    "            score_history.append(round_score)\n",
    "            temp_board_history.append(temp_board)\n",
    "            move_history.append(move_history_str)\n",
    "            if is_gameover:\n",
    "                break\n",
    "        return temp_board_history, move_history, score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e3348-cf29-4ec7-9080-0d28d4ae9a88",
   "metadata": {},
   "source": [
    "# deep q learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23f3ef7-8dc3-4e65-957b-212ea664a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(env):\n",
    "\n",
    "    num_actions = env.findAllMove().shape[0]\n",
    "    shape = env.getGameState().shape\n",
    "\n",
    "    inputs = layers.Input(\n",
    "        shape=(\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #     # Convolutions on the frames on the screen\n",
    "    #     layer1 = layers.Conv2D(\n",
    "    #         filters=32,\n",
    "    #         kernel_size=3,\n",
    "    #         strides=1,\n",
    "    #         padding=\"valid\",\n",
    "    #         activation=\"relu\",\n",
    "    #     )(inputs)\n",
    "\n",
    "    #     layer2 = layers.Conv2D(\n",
    "    #         filters=32,\n",
    "    #         kernel_size=3,\n",
    "    #         strides=1,\n",
    "    #         padding=\"valid\",\n",
    "    #         activation=\"relu\",\n",
    "    #     )(layer1)\n",
    "\n",
    "    #     layer3 = layers.Conv2D(\n",
    "    #         filters=32,\n",
    "    #         kernel_size=3,\n",
    "    #         strides=1,\n",
    "    #         padding=\"valid\",\n",
    "    #         activation=\"relu\",\n",
    "    #     )(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(inputs)\n",
    "\n",
    "    layer5 = layers.Dense(50, activation=\"relu\")(layer4)\n",
    "\n",
    "    layer6 = layers.Dense(50, activation=\"relu\")(layer5)\n",
    "\n",
    "    layer7 = layers.Dense(10, activation=\"relu\")(layer6)\n",
    "\n",
    "    layer8 = layers.Dense(10, activation=\"relu\")(layer7)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer8)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088fe2e-6687-428b-8cce-4cbaa82d07b6",
   "metadata": {},
   "source": [
    "# setup parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd69878-93c9-4fe4-be22-4bd7e4ebf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quick running\n",
    "reduce_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "111369dc-c8a1-45c6-bb92-348a24a5e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning parameter\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "max_episode = 10000000 // reduce_step\n",
    "max_buffer_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c73434-9219-4bb9-a101-c4d4f24d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy parameter\n",
    "epsilon = 1.0\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "\n",
    "# epsilon greedy frame\n",
    "epsilon_random_frames = 50000 // reduce_step\n",
    "epsilon_greedy_frames = 1000000 // reduce_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4f7dbe-4d28-4fae-8295-f7ad624e270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning parameter\n",
    "learning_rate = 0.00025\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8293429f-d617-4933-bb3b-59317ef69009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning parameter\n",
    "update_after_actions = np.max(1, 4 // reduce_step)\n",
    "update_target_network = 10000 // reduce_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb4355-c672-4ff1-b730-a5668370e9f7",
   "metadata": {},
   "source": [
    "# create object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b2bc0-dc0a-46d8-80a5-57b3b20dd2bd",
   "metadata": {},
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc77f99e-06c5-456c-9f8c-fbc41e930c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969b7b15-a1c5-4f7e-aac6-5155407fc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = env.findAllMove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbdc6fe-a2a6-4759-873e-e3928881b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_list.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50109340-9183-432b-b031-a78b68206e28",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78cb3483-c1da-4d4c-978e-4ffd39960ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_model = createModel(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c28ba47-f03c-4fb8-a287-9c5519fa736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = createModel(env)\n",
    "target_model.set_weights(action_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cfb16-a17c-45cf-82a3-07f3743310de",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e8288d7-872d-4caa-a62d-d2451a242636",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    learning_rate=learning_rate,\n",
    "    clipnorm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e27a87-0756-4019-a6b6-4394c80dc183",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af2b40f9-bccf-4cc4-a4e3-50017f6cc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca370806-2e50-46bb-b744-01c4b324f59f",
   "metadata": {},
   "source": [
    "## replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e989117a-56d9-4415-a72a-d8fcd5c1bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"state\",\n",
    "        \"action\",\n",
    "        \"reward\",\n",
    "        \"state_next\",\n",
    "        \"is_gameover\",\n",
    "    ]\n",
    ")\n",
    "episode_reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c2784-995e-4795-a9a9-87ec94df870c",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2fd5bc0-398f-42c7-bd78-cc075e4185cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73e9c4ec-f741-4a4b-998b-bdfb832005ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168392c1cc9b430b84968c65a90115d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 7.20 at episode 5, frame count 100\n",
      "running reward: 8.75 at episode 12, frame count 200\n",
      "running reward: 7.94 at episode 18, frame count 300\n",
      "running reward: 7.58 at episode 24, frame count 400\n",
      "running reward: 8.40 at episode 30, frame count 500\n",
      "running reward: 8.84 at episode 37, frame count 600\n",
      "running reward: 8.91 at episode 43, frame count 700\n",
      "running reward: 9.24 at episode 49, frame count 800\n",
      "running reward: 10.31 at episode 55, frame count 900\n",
      "running reward: 10.31 at episode 61, frame count 1000\n",
      "running reward: 9.91 at episode 68, frame count 1100\n",
      "running reward: 9.31 at episode 75, frame count 1200\n",
      "running reward: 8.80 at episode 82, frame count 1300\n",
      "running reward: 9.09 at episode 88, frame count 1400\n",
      "running reward: 9.02 at episode 95, frame count 1500\n",
      "running reward: 9.01 at episode 101, frame count 1600\n",
      "running reward: 9.05 at episode 107, frame count 1700\n",
      "running reward: 8.76 at episode 114, frame count 1800\n",
      "running reward: 8.61 at episode 121, frame count 1900\n",
      "running reward: 8.28 at episode 127, frame count 2000\n",
      "running reward: 8.17 at episode 132, frame count 2100\n",
      "running reward: 8.06 at episode 139, frame count 2200\n",
      "running reward: 7.75 at episode 146, frame count 2300\n",
      "running reward: 7.76 at episode 152, frame count 2400\n",
      "running reward: 7.10 at episode 158, frame count 2500\n",
      "running reward: 7.03 at episode 164, frame count 2600\n",
      "running reward: 7.52 at episode 170, frame count 2700\n",
      "running reward: 7.82 at episode 177, frame count 2800\n",
      "running reward: 7.64 at episode 184, frame count 2900\n",
      "running reward: 7.76 at episode 190, frame count 3000\n",
      "running reward: 7.88 at episode 196, frame count 3100\n",
      "running reward: 7.94 at episode 201, frame count 3200\n",
      "running reward: 7.63 at episode 208, frame count 3300\n",
      "running reward: 8.01 at episode 214, frame count 3400\n",
      "running reward: 8.22 at episode 220, frame count 3500\n",
      "running reward: 8.08 at episode 227, frame count 3600\n",
      "running reward: 8.06 at episode 233, frame count 3700\n",
      "running reward: 8.28 at episode 240, frame count 3800\n",
      "running reward: 8.33 at episode 246, frame count 3900\n",
      "running reward: 8.44 at episode 252, frame count 4000\n",
      "running reward: 8.20 at episode 258, frame count 4100\n",
      "running reward: 8.13 at episode 264, frame count 4200\n",
      "running reward: 7.96 at episode 270, frame count 4300\n",
      "running reward: 7.74 at episode 277, frame count 4400\n",
      "running reward: 7.50 at episode 284, frame count 4500\n",
      "running reward: 7.40 at episode 290, frame count 4600\n",
      "running reward: 7.00 at episode 296, frame count 4700\n",
      "running reward: 6.83 at episode 303, frame count 4800\n",
      "running reward: 6.89 at episode 310, frame count 4900\n",
      "running reward: 6.80 at episode 316, frame count 5000\n",
      "running reward: 7.00 at episode 322, frame count 5100\n",
      "running reward: 7.14 at episode 328, frame count 5200\n",
      "running reward: 7.29 at episode 335, frame count 5300\n",
      "running reward: 6.99 at episode 341, frame count 5400\n",
      "running reward: 6.82 at episode 347, frame count 5500\n",
      "running reward: 7.49 at episode 354, frame count 5600\n",
      "running reward: 7.57 at episode 360, frame count 5700\n",
      "running reward: 7.51 at episode 367, frame count 5800\n",
      "running reward: 7.43 at episode 373, frame count 5900\n",
      "running reward: 7.61 at episode 380, frame count 6000\n",
      "running reward: 7.62 at episode 386, frame count 6100\n",
      "running reward: 7.92 at episode 392, frame count 6200\n",
      "running reward: 8.04 at episode 398, frame count 6300\n",
      "running reward: 8.62 at episode 404, frame count 6400\n",
      "running reward: 8.53 at episode 411, frame count 6500\n",
      "running reward: 8.82 at episode 417, frame count 6600\n",
      "running reward: 8.78 at episode 423, frame count 6700\n",
      "running reward: 8.45 at episode 430, frame count 6800\n",
      "running reward: 8.39 at episode 436, frame count 6900\n",
      "running reward: 9.02 at episode 442, frame count 7000\n",
      "running reward: 8.86 at episode 448, frame count 7100\n",
      "running reward: 8.67 at episode 453, frame count 7200\n",
      "running reward: 8.67 at episode 459, frame count 7300\n",
      "running reward: 8.78 at episode 465, frame count 7400\n",
      "running reward: 9.00 at episode 471, frame count 7500\n",
      "running reward: 8.76 at episode 479, frame count 7600\n",
      "running reward: 8.87 at episode 485, frame count 7700\n",
      "running reward: 8.57 at episode 491, frame count 7800\n",
      "running reward: 8.22 at episode 498, frame count 7900\n",
      "running reward: 7.73 at episode 504, frame count 8000\n",
      "running reward: 8.63 at episode 509, frame count 8100\n",
      "running reward: 8.27 at episode 515, frame count 8200\n",
      "running reward: 8.49 at episode 521, frame count 8300\n",
      "running reward: 8.73 at episode 527, frame count 8400\n",
      "running reward: 8.65 at episode 534, frame count 8500\n",
      "running reward: 8.62 at episode 540, frame count 8600\n",
      "running reward: 8.27 at episode 546, frame count 8700\n",
      "running reward: 8.01 at episode 552, frame count 8800\n",
      "running reward: 7.63 at episode 558, frame count 8900\n",
      "running reward: 7.55 at episode 565, frame count 9000\n",
      "running reward: 7.50 at episode 571, frame count 9100\n",
      "running reward: 7.59 at episode 577, frame count 9200\n",
      "running reward: 7.70 at episode 583, frame count 9300\n",
      "running reward: 8.15 at episode 588, frame count 9400\n",
      "running reward: 8.40 at episode 594, frame count 9500\n",
      "running reward: 8.70 at episode 600, frame count 9600\n",
      "running reward: 8.51 at episode 606, frame count 9700\n",
      "running reward: 8.14 at episode 613, frame count 9800\n",
      "running reward: 7.76 at episode 619, frame count 9900\n",
      "running reward: 7.50 at episode 626, frame count 10000\n",
      "running reward: 7.98 at episode 632, frame count 10100\n",
      "running reward: 7.66 at episode 638, frame count 10200\n",
      "running reward: 7.84 at episode 644, frame count 10300\n",
      "running reward: 7.91 at episode 651, frame count 10400\n",
      "running reward: 7.88 at episode 656, frame count 10500\n",
      "running reward: 8.44 at episode 663, frame count 10600\n",
      "running reward: 8.26 at episode 669, frame count 10700\n",
      "running reward: 8.33 at episode 675, frame count 10800\n",
      "running reward: 8.43 at episode 680, frame count 10900\n",
      "running reward: 8.49 at episode 687, frame count 11000\n",
      "running reward: 8.07 at episode 693, frame count 11100\n",
      "running reward: 7.95 at episode 699, frame count 11200\n",
      "running reward: 7.49 at episode 706, frame count 11300\n",
      "running reward: 7.58 at episode 713, frame count 11400\n",
      "running reward: 7.86 at episode 719, frame count 11500\n",
      "running reward: 7.78 at episode 726, frame count 11600\n",
      "running reward: 7.57 at episode 732, frame count 11700\n",
      "running reward: 7.45 at episode 739, frame count 11800\n",
      "running reward: 7.50 at episode 746, frame count 11900\n",
      "running reward: 7.77 at episode 752, frame count 12000\n",
      "running reward: 7.41 at episode 758, frame count 12100\n",
      "running reward: 7.50 at episode 765, frame count 12200\n",
      "running reward: 7.53 at episode 771, frame count 12300\n",
      "running reward: 7.43 at episode 778, frame count 12400\n",
      "running reward: 6.57 at episode 784, frame count 12500\n",
      "running reward: 6.57 at episode 790, frame count 12600\n",
      "running reward: 7.02 at episode 796, frame count 12700\n",
      "running reward: 6.96 at episode 803, frame count 12800\n",
      "running reward: 7.05 at episode 809, frame count 12900\n",
      "running reward: 7.07 at episode 815, frame count 13000\n",
      "running reward: 6.95 at episode 821, frame count 13100\n",
      "running reward: 7.20 at episode 827, frame count 13200\n",
      "running reward: 6.92 at episode 833, frame count 13300\n",
      "running reward: 6.83 at episode 839, frame count 13400\n",
      "running reward: 7.07 at episode 846, frame count 13500\n",
      "running reward: 6.89 at episode 852, frame count 13600\n",
      "running reward: 6.97 at episode 858, frame count 13700\n",
      "running reward: 6.83 at episode 864, frame count 13800\n",
      "running reward: 7.15 at episode 871, frame count 13900\n",
      "running reward: 7.04 at episode 878, frame count 14000\n",
      "running reward: 7.16 at episode 884, frame count 14100\n",
      "running reward: 7.22 at episode 890, frame count 14200\n",
      "running reward: 6.85 at episode 896, frame count 14300\n",
      "running reward: 7.09 at episode 902, frame count 14400\n",
      "running reward: 7.29 at episode 909, frame count 14500\n",
      "running reward: 7.51 at episode 915, frame count 14600\n",
      "running reward: 7.33 at episode 922, frame count 14700\n",
      "running reward: 7.60 at episode 928, frame count 14800\n",
      "running reward: 7.81 at episode 935, frame count 14900\n",
      "running reward: 8.16 at episode 941, frame count 15000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m done_sample \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m     71\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(batch_replay[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gameover\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m))),\n\u001b[0;32m     72\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Build the updated Q-values for the sampled future states\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Use the target model for stability\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m future_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_next_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Q value = reward + discount factor * expected future reward\u001b[39;00m\n\u001b[0;32m     80\u001b[0m updated_q_values \u001b[38;5;241m=\u001b[39m rewards_sample \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_max(\n\u001b[0;32m     81\u001b[0m     future_rewards, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1718\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1716\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m   1717\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1718\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1199\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1199\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1200\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    485\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__() is only supported inside of tf.function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    695\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:719\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    715\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    716\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v2(\n\u001b[0;32m    717\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    718\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 719\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m   \u001b[38;5;66;03m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[0;32m    721\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_deleter \u001b[38;5;241m=\u001b[39m IteratorResourceDeleter(\n\u001b[0;32m    722\u001b[0m       handle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[0;32m    723\u001b[0m       deleter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3119\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3118\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3119\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3120\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(device_name):\n",
    "    for episode_count in tqdm(range(max_episode)):\n",
    "\n",
    "        env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "            if timestep == 1:\n",
    "                state = env.getGameState()\n",
    "\n",
    "            frame_count = frame_count + 1\n",
    "\n",
    "            # Use epsilon-greedy for exploration\n",
    "            if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "                # Take random action\n",
    "                action = np.random.choice(num_actions)\n",
    "            else:\n",
    "                # Predict action Q-values\n",
    "                # From environment state\n",
    "                state_tensor = tf.convert_to_tensor(state)\n",
    "                state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "                action_probs = action_model(state_tensor, training=False)\n",
    "                # Take best action\n",
    "                action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "            # Decay probability of taking random action\n",
    "            epsilon = epsilon - (epsilon_interval / epsilon_greedy_frames)\n",
    "            epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state_next, reward, is_gameover = env.move(action_list[action])\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            # Save to replay buffer\n",
    "            replay_dict = {\n",
    "                \"state\": state,\n",
    "                \"action\": action,\n",
    "                \"reward\": reward,\n",
    "                \"state_next\": state_next,\n",
    "                \"is_gameover\": is_gameover,\n",
    "            }\n",
    "            replay_buffer = replay_buffer.append(replay_dict, ignore_index=True)\n",
    "\n",
    "            # Remove for keep buffer size\n",
    "            if replay_buffer.shape[0] > max_buffer_size:\n",
    "                replay_buffer = replay_buffer.iloc[1:, :]\n",
    "                buffer_size = max_buffer_size\n",
    "            else:\n",
    "                buffer_size = replay_buffer.shape[0]\n",
    "\n",
    "            # Update state\n",
    "            state = state_next\n",
    "\n",
    "            # Update action model\n",
    "            if frame_count % update_after_actions == 0 and buffer_size > batch_size:\n",
    "\n",
    "                # Get indices of samples for replay buffers\n",
    "                indices = np.random.choice(\n",
    "                    range(buffer_size), size=batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                batch_replay = replay_buffer.iloc[indices, :]\n",
    "\n",
    "                # Using list comprehension to sample from replay buffer\n",
    "                state_sample = np.array(list(batch_replay[\"state\"]))\n",
    "                action_sample = np.array(list(batch_replay[\"action\"]))\n",
    "                rewards_sample = np.array(list(batch_replay[\"reward\"]))\n",
    "                state_next_sample = np.array(list(batch_replay[\"state_next\"]))\n",
    "                done_sample = tf.convert_to_tensor(\n",
    "                    np.array(list(batch_replay[\"is_gameover\"].astype(float))),\n",
    "                    dtype=np.float32,\n",
    "                )\n",
    "\n",
    "                # Build the updated Q-values for the sampled future states\n",
    "                # Use the target model for stability\n",
    "                future_rewards = target_model.predict(state_next_sample)\n",
    "\n",
    "                # Q value = reward + discount factor * expected future reward\n",
    "                updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                    future_rewards, axis=1\n",
    "                )\n",
    "\n",
    "                # If final frame set the last value to -1\n",
    "                updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "                # Create a mask so we only calculate loss on the updated Q-values\n",
    "                masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Train the model on the states and updated Q-values\n",
    "                    q_values = action_model(state_sample)\n",
    "\n",
    "                    # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                    q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                    # Calculate loss between new Q-value and old Q-value\n",
    "                    loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, action_model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, action_model.trainable_variables))\n",
    "\n",
    "            # Update target model\n",
    "            if frame_count % update_target_network == 0:\n",
    "                # Update the the target network with new weights\n",
    "                target_model.set_weights(action_model.get_weights())\n",
    "                # Log details\n",
    "                template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "                print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "            if is_gameover:\n",
    "                break\n",
    "\n",
    "        # Update running reward\n",
    "        episode_reward_history.append(episode_reward)\n",
    "        if len(episode_reward_history) > 100:\n",
    "            del episode_reward_history[:1]\n",
    "        running_reward = np.mean(episode_reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db6fadab-b26f-440b-bd30-7a13efee57b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabffeb4-11a4-4657-9938-652502dbe29e",
   "metadata": {},
   "source": [
    "# test random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a442e6d-7776-4907-ad93-3647b5d067b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f73d8-1da3-4336-93b3-422a081392d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment(\n",
    "    unique_box_type=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23653ac3-7130-4dd4-a6ce-9a7a62a0747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_board_history, move_history, score_history = env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10c7c9-8053-4cfc-b5d0-d39ed9c91faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.board.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a515088-a678-4844-b7ed-eae7b7f381b9",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe61495-f561-48e4-81e9-8584bb64d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    temp_board = temp_board_history[i]\n",
    "    ax.clear()\n",
    "    ax.pcolor(\n",
    "        temp_board,\n",
    "        cmap=env.box_color_map,\n",
    "        vmin=0,\n",
    "        vmax=env.unique_box_type,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    xticks_label = np.arange(0, env.board_width, 1)\n",
    "    yticks_label = np.arange(0, env.board_height + 1, 1)\n",
    "    # centering of call\n",
    "    ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "    ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "    # label\n",
    "    yticks_label = yticks_label.astype(np.str_)\n",
    "    yticks_label[-1] = \"top\"\n",
    "    ax.set_xticklabels(xticks_label)\n",
    "    ax.set_yticklabels(yticks_label)\n",
    "    ax.set_title(\n",
    "        move_history[i]\n",
    "        + f\"\\n total score : {np.sum(score_history[:i],dtype=int)}, round score : {score_history[i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07687db-07b1-4039-9489-07b3218eb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.close(fig)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    animate,\n",
    "    frames=len(temp_board_history),\n",
    "    interval=500,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d53f4a-8010-49d4-a56b-1181be64d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83877e74-97f7-4933-aa05-2e92e0e532fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ani = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155e587-c857-4eee-b171-407d8c9f33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation as an animated GIF\n",
    "if save_ani:\n",
    "    ani.save(\"random_play_animation.gif\", dpi=300, writer=PillowWriter(fps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140be36d-681c-40c5-96e7-60b246e5dd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
