{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a25cb773-92ba-4285-b1c7-0c2731899d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25b8e4d4-36b9-4c0a-b94f-3292faa95694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7ef4c9e-1373-4526-9649-e498005d05ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.3\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1b1b8c-5e42-40f9-81f5-93e9da6bc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3fd6e92-9e0e-451a-b3a6-3172bbbaca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5cf0c91-d005-4148-b53b-38ab4b65b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04087884-2da8-4406-8e78-e12d10c91d8c",
   "metadata": {},
   "source": [
    "# notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694fa66-470b-40f0-bc88-8543958705e6",
   "metadata": {},
   "source": [
    "## bit definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1180af2a-10d2-4780-8d85-0421b378eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49772e-48b6-42d4-9a91-fea77ea9ad69",
   "metadata": {},
   "source": [
    "| box_value | box_name     |\n",
    "|-----------|--------------|\n",
    "| 0         | Empty        |\n",
    "| 1         | BrownWood    |\n",
    "| 2         | RedWood      |\n",
    "| 3         | GreenWood    |\n",
    "| 4         | BrownLeather |\n",
    "| 5         | BrownPaper   |\n",
    "| 6         | BlueSteel    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de80c4f-0f49-4f94-a3bc-39c93cedc33c",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f50331f6-5161-483e-8c15-1552f76ec948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveTheBoxGameEnvironment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_height=9,\n",
    "        board_width=7,\n",
    "        unique_box_type=6,\n",
    "        p_add_3_box=0.77,\n",
    "        debug=False,\n",
    "    ):\n",
    "\n",
    "        # init parameter\n",
    "        self.board_height = board_height\n",
    "        self.board_width = board_width\n",
    "        self.unique_box_type = unique_box_type\n",
    "        self.p_add_3_box = p_add_3_box\n",
    "        self.debug = debug\n",
    "        self.cumulative_score = 0\n",
    "\n",
    "        # init color_map\n",
    "        box_color_map_list = (\n",
    "            np.array(\n",
    "                [\n",
    "                    [255, 255, 255],\n",
    "                    [169, 133, 92],\n",
    "                    [183, 71, 34],\n",
    "                    [156, 164, 52],\n",
    "                    [102, 65, 38],\n",
    "                    [189, 140, 61],\n",
    "                    [149, 165, 168],\n",
    "                ]\n",
    "            )\n",
    "            / 255\n",
    "        )\n",
    "        self.box_color_map = LinearSegmentedColormap.from_list(\n",
    "            \"box_color_map\",\n",
    "            box_color_map_list,\n",
    "            box_color_map_list.shape[0],\n",
    "        )\n",
    "\n",
    "        # init game\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def reset(self):\n",
    "        self.createBoard()\n",
    "        self.createAboveBoard()\n",
    "        self.addAboveBox()\n",
    "        self.aboveIncludeFall()\n",
    "\n",
    "    def createBoard(self):\n",
    "        self.board = np.zeros(\n",
    "            (self.board_height, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def createAboveBoard(self):\n",
    "        self.above_board = np.zeros(\n",
    "            (1, self.board_width),\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "\n",
    "    def addAboveBox(self):\n",
    "        number_of_box_above = np.random.choice(\n",
    "            [3, 4], p=[self.p_add_3_box, 1 - self.p_add_3_box]\n",
    "        )\n",
    "        box_value = np.random.randint(\n",
    "            low=1,\n",
    "            high=self.unique_box_type + 1,\n",
    "            size=number_of_box_above,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox = np.zeros(\n",
    "            self.board_width,\n",
    "            dtype=np.int8,\n",
    "        )\n",
    "        aboveBox[:number_of_box_above] = 1\n",
    "        aboveBox[aboveBox == 1] = box_value\n",
    "        np.random.shuffle(aboveBox)\n",
    "        self.above_board = aboveBox.reshape((1, self.board_width))\n",
    "\n",
    "    def aboveIncludeFall(self):\n",
    "        is_gameover = self.checkIsGameover()\n",
    "        if is_gameover:\n",
    "            return is_gameover\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        self.above_board = temp_board[-1].reshape((1, self.board_width)).copy()\n",
    "        self.board = temp_board[:-1].copy()\n",
    "        self.addAboveBox()\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        if self.checkIsEmptyBoard():\n",
    "            return self.aboveIncludeFall()\n",
    "        return False\n",
    "\n",
    "    # game mechanism\n",
    "    def fall(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        for y in range(self.board_width):\n",
    "            x = 0\n",
    "            is_fall = False\n",
    "            while x < temp_board.shape[0] - 1:\n",
    "                # skip floor and run loop\n",
    "                x += 1\n",
    "                # check below is empty\n",
    "                if not self.checkIsEmpty(temp_board[x, y]) and self.checkIsEmpty(\n",
    "                    temp_board[x - 1, y]\n",
    "                ):\n",
    "                    # fall\n",
    "                    temp_board[x - 1, y] = temp_board[x, y].copy()\n",
    "                    temp_board[x, y] = 0\n",
    "                    is_fall = True\n",
    "\n",
    "                # reset and fall again\n",
    "                if is_fall and (x >= temp_board.shape[0] - 1):\n",
    "                    x = 0\n",
    "                    is_fall = False\n",
    "        return temp_board\n",
    "\n",
    "    def remove(self, temp_board):\n",
    "        temp_board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            self.displayTempBoard(temp_board)\n",
    "        remove_list = self.check(temp_board)\n",
    "        numbers_of_remove = np.unique(remove_list, axis=0).shape[0]\n",
    "        if self.debug:\n",
    "            print(f\"remove_list : {remove_list}\")\n",
    "            print(f\"number of remove box (remove): {numbers_of_remove}\")\n",
    "        numbers_of_remove_recursive = []\n",
    "        if len(remove_list) > 0:\n",
    "            for remove_index in remove_list:\n",
    "                temp_board[remove_index[0], remove_index[1]] = 0\n",
    "            temp_board = self.fall(temp_board)\n",
    "            temp_board, numbers_of_remove_recursive = self.remove(temp_board)\n",
    "        return temp_board, [numbers_of_remove] + numbers_of_remove_recursive\n",
    "\n",
    "    def check(self, temp_board):\n",
    "        remove_list = []\n",
    "        for x in range(temp_board.shape[0]):\n",
    "            remove_list = remove_list + self.checkRow(temp_board, x)\n",
    "\n",
    "        for y in range(temp_board.shape[1]):\n",
    "            remove_list = remove_list + self.checkCol(temp_board, y)\n",
    "        return remove_list\n",
    "\n",
    "    def checkRow(self, temp_board, x):\n",
    "        count = 1\n",
    "        last_found = temp_board[x, 0]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "\n",
    "        for y in range(1, temp_board.shape[1]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if y >= temp_board.shape[1] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x, y - 1]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x, y - i - 1])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def checkCol(self, temp_board, y):\n",
    "        count = 1\n",
    "        last_found = temp_board[0, y]\n",
    "        remove_list = []\n",
    "        is_change = False\n",
    "        for x in range(1, temp_board.shape[0]):\n",
    "            if self.checkIsSameColor(last_found, temp_board[x, y]):\n",
    "                if self.checkIsEmpty(temp_board[x, y]):\n",
    "                    continue\n",
    "                else:\n",
    "                    count += 1\n",
    "                    # end check index\n",
    "                    if x >= temp_board.shape[0] - 1:\n",
    "                        is_change = True\n",
    "            else:\n",
    "                last_found = temp_board[x, y]\n",
    "                is_change = True\n",
    "            if is_change:\n",
    "                if not self.checkIsEmpty(temp_board[x - 1, y]):\n",
    "                    if count >= 3:\n",
    "                        for i in range(count):\n",
    "                            remove_index = np.array([x - i - 1, y])\n",
    "                            remove_list.append(remove_index)\n",
    "                count = 1\n",
    "                is_change = False\n",
    "        return remove_list\n",
    "\n",
    "    def updateBoard(self):\n",
    "        temp_board = self.board.copy()\n",
    "        temp_board = self.fall(temp_board)\n",
    "        temp_board, numbers_of_remove = self.remove(temp_board)\n",
    "        self.board = temp_board.copy()\n",
    "        if self.debug:\n",
    "            print(f\"numbers_of_remove (updateBoard): {numbers_of_remove}\")\n",
    "        return self.calculatePoint(numbers_of_remove)\n",
    "\n",
    "    def calculatePoint(self, numbers_of_remove_box):\n",
    "        numbers_of_remove_box = np.array(numbers_of_remove_box)\n",
    "        points = []\n",
    "        for t, number_of_box in enumerate(numbers_of_remove_box):\n",
    "            if number_of_box > 0:\n",
    "                summation = 0\n",
    "                for i in range(0, number_of_box - 3 + 1):\n",
    "                    summation = summation + i\n",
    "                if t == 0:\n",
    "                    point = number_of_box + summation\n",
    "                else:\n",
    "                    main_point = numbers_of_remove_box[t] * np.sum(\n",
    "                        numbers_of_remove_box[:t]\n",
    "                    )\n",
    "                    point = main_point + (number_of_box - 3) + summation\n",
    "            else:\n",
    "                point = 0\n",
    "            points.append(point)\n",
    "        return np.sum(points)\n",
    "\n",
    "    # utility\n",
    "    def displayBoard(self):\n",
    "        # create virtual board\n",
    "        temp_board = np.vstack((self.board, self.above_board)).copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height + 1, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        yticks_label = yticks_label.astype(np.str)\n",
    "        yticks_label[-1] = \"above\"\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def displayTempBoard(self, temp_board):\n",
    "        # create virtual board\n",
    "        temp_board = temp_board.copy()\n",
    "        # plot virtual board\n",
    "        fig = plt.figure(figsize=(3.5, 4.5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.pcolor(\n",
    "            temp_board,\n",
    "            cmap=self.box_color_map,\n",
    "            vmin=0,\n",
    "            vmax=self.unique_box_type,\n",
    "            edgecolors=\"k\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        xticks_label = np.arange(0, self.board_width, 1)\n",
    "        yticks_label = np.arange(0, self.board_height, 1)\n",
    "        # centering of call\n",
    "        ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "        ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "        # label\n",
    "        ax.set_xticklabels(xticks_label)\n",
    "        ax.set_yticklabels(yticks_label)\n",
    "        plt.show()\n",
    "\n",
    "    def checkIsEmpty(self, box):\n",
    "        return box == 0\n",
    "\n",
    "    def checkIsSameColor(self, box1, box2):\n",
    "        return box1 == box2\n",
    "\n",
    "    def checkIsEmptyBoard(self):\n",
    "        return (self.board == 0).all()\n",
    "\n",
    "    # action\n",
    "    def findAllMove(self):\n",
    "        all_move_index = []\n",
    "        row_arange = np.arange(0, self.board_height, 1)\n",
    "        col_arange = np.arange(0, self.board_width, 1)\n",
    "        row_index, col_index = np.meshgrid(row_arange, col_arange)\n",
    "        point_index = np.vstack((row_index.reshape(-1), col_index.reshape(-1))).T\n",
    "\n",
    "        for index in point_index:\n",
    "\n",
    "            if index[1] < self.board_width - 1:\n",
    "                # move_right\n",
    "                move_right_index = np.array([index, index + np.array([0, 1])])\n",
    "                all_move_index.append(move_right_index)\n",
    "\n",
    "            if index[0] < self.board_height - 1:\n",
    "                # move_up\n",
    "                move_up_index = np.array([index, index + np.array([1, 0])])\n",
    "                all_move_index.append(move_up_index)\n",
    "\n",
    "        return np.array(all_move_index)\n",
    "\n",
    "    def findNotDuplicateMove(self):\n",
    "        all_move_index = self.findAllMove()\n",
    "\n",
    "        not_duplicate_move_index = []\n",
    "        for move_index in all_move_index:\n",
    "            point_a_index = move_index[0]\n",
    "            point_a_value = self.board[point_a_index[0], point_a_index[1]]\n",
    "\n",
    "            point_b_index = move_index[1]\n",
    "            point_b_value = self.board[point_b_index[0], point_b_index[1]]\n",
    "\n",
    "            if not self.checkIsSameColor(point_a_value, point_b_value):\n",
    "                not_duplicate_move_index.append(move_index)\n",
    "\n",
    "        return np.array(not_duplicate_move_index)\n",
    "\n",
    "    def move(self, move_index):\n",
    "        before_cumulative_score = self.cumulative_score\n",
    "        # collect value\n",
    "        point_a_index = move_index[0]\n",
    "        point_a_value = self.board[point_a_index[0], point_a_index[1]].copy()\n",
    "\n",
    "        point_b_index = move_index[1]\n",
    "        point_b_value = self.board[point_b_index[0], point_b_index[1]].copy()\n",
    "\n",
    "        # move\n",
    "        self.board[point_a_index[0], point_a_index[1]] = point_b_value\n",
    "        self.board[point_b_index[0], point_b_index[1]] = point_a_value\n",
    "        score = self.updateBoard()\n",
    "        self.cumulative_score = self.cumulative_score + score\n",
    "        is_gameover = self.aboveIncludeFall()\n",
    "        return (\n",
    "            self.getGameState(),\n",
    "            self.cumulative_score - before_cumulative_score,\n",
    "            is_gameover,\n",
    "        )\n",
    "\n",
    "    def checkIsGameover(self):\n",
    "        for y in range(self.board.shape[1]):\n",
    "            full_board = (self.board[:, y] != 0).all()\n",
    "            full_above_board = (self.above_board[:, y] != 0).all()\n",
    "            if full_board and full_above_board:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Reinforcement\n",
    "    def getGameState(self):\n",
    "        return np.vstack((self.board, self.above_board)).copy()\n",
    "\n",
    "    def play(self):\n",
    "        temp_board_history = []\n",
    "        move_history = []\n",
    "        score_history = []\n",
    "        for i in range(200):\n",
    "            temp_board = self.getGameState()\n",
    "            move_index = self.findNotDuplicateMove()\n",
    "            select_move_index = np.random.randint(move_index.shape[0])\n",
    "            move_history_str = f\"move {i+1} : move from {move_index[select_move_index,0,:]} to {move_index[select_move_index,1,:]}\"\n",
    "            _, round_score, is_gameover = self.move(move_index[select_move_index])\n",
    "            score_history.append(round_score)\n",
    "            temp_board_history.append(temp_board)\n",
    "            move_history.append(move_history_str)\n",
    "            if is_gameover:\n",
    "                break\n",
    "        return temp_board_history, move_history, score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e3348-cf29-4ec7-9080-0d28d4ae9a88",
   "metadata": {},
   "source": [
    "# deep q learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e23f3ef7-8dc3-4e65-957b-212ea664a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(env):\n",
    "\n",
    "    num_actions = env.getGameState().shape[0]\n",
    "    shape = env.getGameState().shape\n",
    "\n",
    "    inputs = layers.Input(\n",
    "        shape=(\n",
    "            shape[0],\n",
    "            shape[1],\n",
    "            1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "    )(inputs)\n",
    "\n",
    "    layer2 = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "    )(layer1)\n",
    "\n",
    "    layer3 = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"valid\",\n",
    "        activation=\"relu\",\n",
    "    )(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(shape[0] * shape[1], activation=\"relu\")(layer4)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1088fe2e-6687-428b-8cce-4cbaa82d07b6",
   "metadata": {},
   "source": [
    "# setup parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "111369dc-c8a1-45c6-bb92-348a24a5e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q learning parameter\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 10000\n",
    "max_episode = 10000000\n",
    "max_buffer_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95c73434-9219-4bb9-a101-c4d4f24d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy parameter\n",
    "epsilon = 1.0\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "\n",
    "# epsilon greedy frame\n",
    "epsilon_random_frames = 50000\n",
    "epsilon_greedy_frames = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca4f7dbe-4d28-4fae-8295-f7ad624e270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning parameter\n",
    "learning_rate = 0.00025\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8293429f-d617-4933-bb3b-59317ef69009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep q learning parameter\n",
    "update_after_actions = 4\n",
    "update_target_network = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb4355-c672-4ff1-b730-a5668370e9f7",
   "metadata": {},
   "source": [
    "# create object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b2bc0-dc0a-46d8-80a5-57b3b20dd2bd",
   "metadata": {},
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc77f99e-06c5-456c-9f8c-fbc41e930c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "969b7b15-a1c5-4f7e-aac6-5155407fc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = env.findAllMove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbbdc6fe-a2a6-4759-873e-e3928881b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = action_list.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50109340-9183-432b-b031-a78b68206e28",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78cb3483-c1da-4d4c-978e-4ffd39960ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_model = createModel(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c28ba47-f03c-4fb8-a287-9c5519fa736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = createModel(env)\n",
    "target_model.set_weights(action_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cfb16-a17c-45cf-82a3-07f3743310de",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e8288d7-872d-4caa-a62d-d2451a242636",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    learning_rate=learning_rate,\n",
    "    clipnorm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e27a87-0756-4019-a6b6-4394c80dc183",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af2b40f9-bccf-4cc4-a4e3-50017f6cc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca370806-2e50-46bb-b744-01c4b324f59f",
   "metadata": {},
   "source": [
    "## replay memmory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e989117a-56d9-4415-a72a-d8fcd5c1bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"state\",\n",
    "        \"action\",\n",
    "        \"reward\",\n",
    "        \"state_next\",\n",
    "        \"is_gameover\",\n",
    "    ]\n",
    ")\n",
    "episode_reward_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c2784-995e-4795-a9a9-87ec94df870c",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2fd5bc0-398f-42c7-bd78-cc075e4185cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "running_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73e9c4ec-f741-4a4b-998b-bdfb832005ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 7)\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model_5/conv2d_15/Conv2D (defined at <ipython-input-64-118f15c85549>:76) ]] [Op:__inference_predict_function_1138]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-118f15c85549>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# Use the target model for stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_next_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# Q value = reward + discount factor * expected future reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1720\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1722\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1723\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1724\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    954\u001b[0m               *args, **kwds)\n\u001b[0;32m    955\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m       return self._concrete_stateful_fn._call_flat(\n\u001b[0m\u001b[0;32m    957\u001b[0m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model_5/conv2d_15/Conv2D (defined at <ipython-input-64-118f15c85549>:76) ]] [Op:__inference_predict_function_1138]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "for episode_count in range(max_episode):\n",
    "\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "\n",
    "        if timestep == 1:\n",
    "            state = env.getGameState()\n",
    "\n",
    "        frame_count = frame_count + 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon = epsilon - (epsilon_interval / epsilon_greedy_frames)\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, is_gameover = env.move(action_list[action])\n",
    "        episode_reward = episode_reward + reward\n",
    "\n",
    "        # Save to replay buffer\n",
    "        replay_dict = {\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward,\n",
    "            \"state_next\": state_next,\n",
    "            \"is_gameover\": is_gameover,\n",
    "        }\n",
    "        replay_buffer = replay_buffer.append(replay_dict, ignore_index=True)\n",
    "\n",
    "        # Remove for keep buffer size\n",
    "        if replay_buffer.shape[0] > max_buffer_size:\n",
    "            replay_buffer = replay_buffer.iloc[1:, :]\n",
    "            buffer_size = max_buffer_size\n",
    "        else:\n",
    "            buffer_size = replay_buffer.shape[0]\n",
    "\n",
    "        # Update state\n",
    "        state = state_next\n",
    "\n",
    "        # Update action model\n",
    "        if frame_count % update_after_actions == 0 and buffer_size > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(\n",
    "                range(buffer_size), size=batch_size, replace=False\n",
    "            )\n",
    "\n",
    "            batch_replay = replay_buffer.iloc[indices, :]\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array(list(batch_replay[\"state\"]))\n",
    "            action_sample = np.array(list(batch_replay[\"action\"]))\n",
    "            rewards_sample = np.array(list(batch_replay[\"reward\"]))\n",
    "            state_next_sample = np.array(list(batch_replay[\"state_next\"]))\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                np.array(list(batch_replay[\"is_gameover\"].astype(float)))\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            print(state_next_sample.shape)\n",
    "            future_rewards = target_model.predict(state_next_sample)\n",
    "\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = action_model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, action_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, action_model.trainable_variables))\n",
    "\n",
    "        # Update target model\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # Update the the target network with new weights\n",
    "            target_model.set_weights(action_model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        if is_gameover:\n",
    "            break\n",
    "\n",
    "    # Update running reward\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabffeb4-11a4-4657-9938-652502dbe29e",
   "metadata": {},
   "source": [
    "# test random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a442e6d-7776-4907-ad93-3647b5d067b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f73d8-1da3-4336-93b3-422a081392d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveTheBoxGameEnvironment(\n",
    "    unique_box_type=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23653ac3-7130-4dd4-a6ce-9a7a62a0747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_board_history, move_history, score_history = env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10c7c9-8053-4cfc-b5d0-d39ed9c91faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.board.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a515088-a678-4844-b7ed-eae7b7f381b9",
   "metadata": {},
   "source": [
    "## animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe61495-f561-48e4-81e9-8584bb64d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    temp_board = temp_board_history[i]\n",
    "    ax.clear()\n",
    "    ax.pcolor(\n",
    "        temp_board,\n",
    "        cmap=env.box_color_map,\n",
    "        vmin=0,\n",
    "        vmax=env.unique_box_type,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    xticks_label = np.arange(0, env.board_width, 1)\n",
    "    yticks_label = np.arange(0, env.board_height + 1, 1)\n",
    "    # centering of call\n",
    "    ax.set_xticks(xticks_label + 0.5, minor=False)\n",
    "    ax.set_yticks(yticks_label + 0.5, minor=False)\n",
    "    # label\n",
    "    yticks_label = yticks_label.astype(np.str)\n",
    "    yticks_label[-1] = \"top\"\n",
    "    ax.set_xticklabels(xticks_label)\n",
    "    ax.set_yticklabels(yticks_label)\n",
    "    ax.set_title(\n",
    "        move_history[i]\n",
    "        + f\"\\n total score : {np.sum(score_history[:i],dtype=int)}, round score : {score_history[i]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07687db-07b1-4039-9489-07b3218eb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5, 4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.close(fig)\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    animate,\n",
    "    frames=len(temp_board_history),\n",
    "    interval=500,\n",
    "    repeat=False,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d53f4a-8010-49d4-a56b-1181be64d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import PillowWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83877e74-97f7-4933-aa05-2e92e0e532fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ani = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155e587-c857-4eee-b171-407d8c9f33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation as an animated GIF\n",
    "if save_ani:\n",
    "    ani.save(\"random_play_animation.gif\", dpi=300, writer=PillowWriter(fps=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140be36d-681c-40c5-96e7-60b246e5dd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
